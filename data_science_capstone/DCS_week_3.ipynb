{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb83fe19",
   "metadata": {},
   "source": [
    "# Week 3 Overview\n",
    "This week, you will examine the following techniques to use with linear regression: forward and backward selection, principal component regression (PCR), and partial least squares regression (PLSR).\n",
    "\n",
    "### Learning Objectives\n",
    "At the end of this week, you should be able to: \n",
    "- Discern the appropriate conditions for using forward and backward selection, PCR, and PLSR on your project’s dataset. \n",
    "- Perform forward and backward selection to analyze your project’s dataset. \n",
    "- Perform PCR and PLSR to analyze your project’s dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d068b06",
   "metadata": {},
   "source": [
    "## 3.1 Lesson: Forward and Backward Selection\n",
    "\n",
    "### Forward Selection\n",
    "Recall that forward and backward selection are stepwise methods for selecting variables in a regression model. Forward selection starts with no predictors and adds them one by one based on which improves the model most, while backward selection starts with all predictors and removes the least useful ones step by step. \n",
    "\n",
    "In forward selection, we perform linear regression by first considering each individual feature as the sole relevant feature in the regression. We test which regression leads to the least loss and stick with that feature. Then, we move on to pick the next feature. For example, suppose our three features are the mass, diameter, and price of a cookie. We want to predict how tasty the cookie is. Then we’d start by performing a regression where mass is the only feature and try to predict tastiness. We’d perform another regression for just diameter and another for just price.\n",
    "\n",
    "Which of these is the best regression (according to some metric)? Let’s say mass is the best feature at level one. Now we have to do two more regressions: mass and diameter or mass and price. We compare these two to each other, giving us the best regression at level two. We don’t try diameter and price, because our level one regression was mass, so we have to include mass. At level three, there’s only one possibility: mass and diameter and price. So that’s our best regression at level three.  \n",
    "\n",
    "We keep going until a certain criterion is met: \n",
    "- We may predetermine how many features we want to use, or \n",
    "- We could test the model on the validation set and stop adding features when the validation loss stops getting better. \n",
    "\n",
    "### Backwards Selections\n",
    "In backward selection, we start with all possible features and then start subtracting them one at a time. Each time we subtract a feature, the performance on the training data will get worse, but the performance on the validation data might improve. Again, we could keep going until: \n",
    "- We reach a predetermined number of features, or \n",
    "- We test the model on the validation set and stop removing features when the validation loss stops getting better. \n",
    "\n",
    "### Limitations of Forward and Backward Selection\n",
    "Note that neither of these approaches considers all possible feature combinations. If there are $N$ features, then you consider $N$ features in the first set, then $N-1$ (because you've already included one of them), then $N-2$, and so on. In total - you can consider at most $N$ choose 2, or $\\frac{N(N-1)}{2}$ combinations, the same is true for backward selection.\n",
    "\n",
    "For example, if the features are $A$, $B$, and $C$, and you pick $A$ first in forward selection, then you have tried $B$ and $C$ alone, but you will never reach the combination ($B,C$) without $A$. (in fact, this combination $(B,C)$ is the conly combination you will miss in this case). There are seven combinations, and you will reach six of them. \n",
    "\n",
    "Likewise, if you remove $A$ first in backward selection, then you have tried $(A,B)$, $(A,C)$, and $(B,C)$., but you will never reach the feature set that contains $A$ alone. (Again, $A$ is the *only* combination you will miss in this case.)\n",
    "\n",
    "Although in the above examples only one feature is missed, with large datasets the problem gets worse. \n",
    "- With 3 features there are 7 combinations, but with 10 features there are 1,023 combinations. \n",
    "- In forward/backward selection you'd only reach $\\frac{10 \\cdot 11}{2} \\; =\\; 55$ of them\n",
    "\n",
    "The idea is that you can (in principle) reach any combination as long as the one-at-a-time process does a good job of picking the best combination of features. \n",
    "\n",
    "Also note that if you compute the p-values of these regressions in the usual way, these p-values cannot be trusted. That’s because if you test 55 different regressions and pick the best one, it could be that the regression was good just by luck. In fact, getting at least one p-value of 0.02 (close to $\\frac{1}{55}$) would be expected for this number of regressions, even if none of them is measuring anything real, assuming their p-values are independent of one another. \n",
    "\n",
    "### Think About It\n",
    "In forward selection with four features ($A$, $B$, $C$, and $D$), which feature combinations do you miss if you add the features in the order: $A$, $B$, $C$, and then $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b812b",
   "metadata": {},
   "source": [
    "## Principal Component Regression (PCR)\n",
    "\n",
    "In a PCR, we perform a principal component analysis (PCA) to transform the data. Then we perform a linear regression on the transformed samples.\n",
    "\n",
    "For example, if $X$ is:\n",
    "$$\n",
    "X \\;=\\; \\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First, we center the data by subtracting the mean of each column:\n",
    "$$\n",
    "X^* \\;=\\; \\begin{bmatrix}\n",
    "0.5 & -0.5 \\\\\n",
    "-0.5 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we want the eigenvectors of\n",
    "$$\n",
    "\\frac{1}{n - 1} \\, X^{*T} X^*,\n",
    "$$\n",
    "where $n$ is the number of features in the problem (the number of columns of $X$). In our case, $n = 2$, so\n",
    "$$\n",
    "\\frac{1}{2 - 1} \\, X^{*T} X^* \\;=\\; \\begin{bmatrix}\n",
    "0.5 & -0.5 \\\\\n",
    "-0.5 & 0.5\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The principal component vectors are then $[1,\\,1]$ (with eigenvalue 0) and $[1,\\,-1]$ (with eigenvalue 1). With PCR, we’d use the latter (with the larger eigenvalue) as the first principal component, in the form:\n",
    "$$\n",
    "\\bigl[\\sqrt{0.5},\\; -\\sqrt{0.5}\\bigr].\n",
    "$$\n",
    "\n",
    "The first score vector then comes from taking the inner product of the first row (a sample, after centering) with each principal component; for example:\n",
    "$$\n",
    "[\\,0.5,\\; -0.5\\,] \\;\\cdot\\; \\bigl[\\sqrt{0.5},\\, -\\sqrt{0.5}\\bigr] \\;=\\; \\sqrt{0.5}\n",
    "\\quad\\text{and}\\quad\n",
    "[\\,0.5,\\; -0.5\\,] \\;\\cdot\\; \\bigl[\\sqrt{0.5},\\, \\sqrt{0.5}\\bigr] \\;=\\; 0.\n",
    "$$\n",
    "Thus, the score vector for the first sample is $[\\sqrt{0.5},\\,0]$.\n",
    "\n",
    "The score vector of the second sample is calculated similarly as $[-\\sqrt{0.5},\\,0]$. The second component is not useful as a predictor in that it always has the same value (hence the eigenvalue 0).\n",
    "\n",
    "> **Note:** If you use *all* of the new features (the principal components), the linear regression on the transformed samples does exactly the same thing as regression on the original samples; only the coefficients are adjusted to account for the linear transformation from the base features to PCA scores. The only way to get a different answer is if you remove some of the features. This could be helpful if:\n",
    ">\n",
    "> - Using all of the features would mean overfitting. (Performance on the validation set improves when features are removed.) This is especially likely with the PCA scores if some of the scores have very low variance.\n",
    "> - Fewer features would lead to better explainability (e.g., you can graph two features, but you can’t graph 100 features).\n",
    "\n",
    "This approach assumes that the high-variance directions selected by the PCA are the correct ones. It fails if a low-variance direction is what’s needed. That is, there could be a direction in which the data features do not vary much, but that small variance triggers a large deviation in the target or outcome variable.\n",
    "\n",
    "In PCR, as with PCA, it can be helpful to standardize the features. That is, each feature should be rescaled so as to have mean zero and standard deviation one. Otherwise, the importance of a feature could be very arbitrary and depend on the unit you choose (miles vs. feet).\n",
    "\n",
    "---\n",
    "\n",
    "## Partial Least Squares Regression (PLSR)\n",
    "\n",
    "With PLSR, we perform dimensionality reduction similarly to PCR, but with a key difference: instead of selecting components that capture the most variance in the features $X$, we select components that maximize the covariance between the projected features (score vectors) and the outcome $Y$.\n",
    "\n",
    "This approach differs from PCR in that it explicitly incorporates the relationship between the features and the target variable. While PCR may retain components that explain a lot of variance in $X$ but are uninformative for predicting $Y$, PLS prioritizes directions in the data that are most useful for prediction. This is particularly helpful when some of the components with large variance are not actually relevant to the outcome.\n",
    "\n",
    "---\n",
    "\n",
    "## Think About It\n",
    "\n",
    "- Describe a situation in which a particular feature (or principal component) would have low variance but would still be important in making a prediction. (For instance, imagine that we don’t standardize the data.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699ebda",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
