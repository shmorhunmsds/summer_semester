{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8542b9",
   "metadata": {},
   "source": [
    "# Welcome to Week 4!\n",
    "This week, you will examine the following techniques: logistic regression and feature scaling.\n",
    "\n",
    "### Learning Objectives\n",
    "At the end of this week, you should be able to: \n",
    "- Perform logistic regression to analyze a dataset. \n",
    "- Scale a dataset's features by normalizing or standardizing the samples. \n",
    "- Design an optimal modeling strategy that implements logistic regression and/or feature scaling. \n",
    "- Explain why it can be helpful to scale features before performing linear or logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edbcecf",
   "metadata": {},
   "source": [
    "## 4.1 Lesson: Logistic Regression and Feature Scaling\n",
    "\n",
    "### Logistic Regression\n",
    "Suppose you want to predict binary categorical data (0 or 1) from some feature. For example, you might measure the price of steak, milk, eggs, and bacon across 1,000 grocery stores, and you want to predict whether the store has an organic foods aisle. Your hypothesis is that stores with high prices have an organic foods aisle, while stores with lower prices don’t.\n",
    "\n",
    "Then:\n",
    "\n",
    "$Y \\; = \\; 0$ (no organic foods aisle)$\n",
    "\n",
    "While:\n",
    "\n",
    "$X_1, X_2, X_3$ and $X_4$ are the prices of the four foods listed.\n",
    "\n",
    "You could use linear regression directly:\n",
    "\n",
    "$Y \\; = \\; \\beta_0 + \\sum_{i} \\beta_0 X_i$\n",
    "\n",
    "But this approach is flawed. Of course, no matter how we do this, we may end up with a $Y$ that is not 0 or 1. We could interpret the number as a probability between 0 and 1, but even then it is difficult to ensure that the regression results in a number between 0 and 1.\n",
    "\n",
    "Instead, we can use logistic regression, a method that predicts the probability of an outcome with two possible classes using a formula based on input features and the logistic function. We can use logistic regression to produce a result that is either 0 or 1:\n",
    "\n",
    "$\\text{Logit} (Y) \\; = \\; log \\left( \\frac {Y}{1 - Y}\\right) \\; = \\; \\beta_0 + \\sum_i \\beta_i X_i$\n",
    "\n",
    "I will right `LINEXP` to mean $\\beta_0 + \\sum_i\\beta_iX_i$\n",
    "\n",
    "This yields: $\\frac{Y}{1 - Y} \\; = \\; \\text{exp (LINEXP)}$\n",
    "\n",
    "$Y \\; = \\; \\text{exp (LINEXP)} (1 - Y)$\n",
    "\n",
    "$Y(1 + \\text{exp(LINEXP)}) \\; = \\; \\text{exp(LINEXP)}$\n",
    "\n",
    "$Y \\; = \\; \\frac{\\text{exp(LINEXP)}}{1 + \\text{exp(LINEXP)}} \\; = \\; \\frac{1}{1 + \\text{exp(- LINEXP)}}$\n",
    "\n",
    "the cross-entropy loss is then:\n",
    "\n",
    "$\\text{Loss} \\; = \\; - \\frac{1}{N} \\sum(y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i))$\n",
    "\n",
    "Where $\\hat{p}_i$ is the $ith$ estimated probability for the $ith$ sample,\n",
    "\n",
    "$\\frac{1}{1 + \\exp (-\\text{LINEXP})}$\n",
    "\n",
    "and $y_i$ is the true $Y$ value for the $i$th sample.\n",
    "\n",
    "We can combine this with Lasso or Ridge Regression. We might imagine writing: \n",
    "\n",
    "$ \\text{loss} \\; = \\; -\\frac{1}{N}\\sum(y_i\\log(\\hat{p}_i) + (1 - y_i)\\log(1 - \\hat{p}_i) + \\lambda \\sum | \\beta_i |)$\n",
    "___\n",
    "\n",
    "This can be sensitive to outliers. For example:\n",
    "\n",
    "If $\\text{LINEXP} \\; = \\; -100$, then the value of $\\log(p_i)$ is about $\\log(\\exp(-1000)) \\; = \\; -1000$.\n",
    "\n",
    "So, the \"log\" dependence doesn't really help to reduce the effect of the outliers. \n",
    "\n",
    "Logistic regression is also vulnerable to multicollinearity, just as linear regression is (and for the same reasons):\n",
    "\n",
    "If $X_1 and $X_2$ are about the same, we can't always tell whether coefficients should be $2X_1 - X_2$ or $1000X_1 - 999X_2$.\n",
    "___\n",
    "Another way of putting this is that logistic regression requires independent samples, e.g., not a correlated time series. For example, imagine that two stocks are measured over time and that the stock at time $t + 1$ is usually about equal to the stock at time $t$. Then you could have values:\n",
    "\n",
    "`stock_1 = [1, 0.99, 0.98, 0.99, 1, 0.99, 1.01]`\n",
    "\n",
    "`stock_2 = [3, 3.01, 2.98, 2.97, 3, 3.01, 2.99]`\n",
    "\n",
    "In this case, we may have hundreds of samples, but we don't have hundreds of independent examples of the relationship between `stock_1` and `stock_2`. In the data shown, we really only have one example: what happens when `stock_1 = 1` and `stock_2 = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a646c",
   "metadata": {},
   "source": [
    "## Feature Scaling:\n",
    "It can be helpful to scale the features before performing linear regression, logistic regression, or a wide variety of other models. \n",
    "\n",
    "**Feature scaling** is the process of normalizing or standardizing the range of independent variables (features) in your data: \n",
    "- It ensures that each feature contributes equally to a model's performance by transforming the data so that it lies within a comparable scale. \n",
    "\n",
    "**Normalization** and **standardization** are two ways to ensure that L1 regression (Lasso) and L2 regression (Ridge) are applied uniformly across all features.\n",
    "\n",
    "**Normalization** is min-max scaling; each sample is assigned a value between 0 and 1. \n",
    "\n",
    "$$ X \\; = \\; \\frac{X - X_{min}}{X_{max} - X_{min}} $$\n",
    "\n",
    "**Standardization** is based on mean and standard deviation; the samples are recentered so that the mean is 0 and the standard deviation is 1:\n",
    "\n",
    "$$ X \\; = \\; \\frac{X - X_{mean}}{X_{std}}$$\n",
    "\n",
    "___\n",
    "To illustrate how this is useful:\n",
    "\n",
    "Suppose we are trying to predict customer behavior at a grocery store based on the prices of the items: \n",
    "- One price is for a gallon of milk, and another is for a high-quality steak. \n",
    "- The steak costs several times as much as the milk, so the coefficient of the steak’s price is likely less than that of the milk. \n",
    "\n",
    "If we apply lasso regression with the terms \n",
    "\n",
    "$$| \\beta_{milk} | + | \\beta_{steak} | $$ \n",
    "\n",
    "we are asking the model to reduce the two coefficients equally, which does not make sense. However, if we normalize or standardize the prices, it will make sense to reduce the coefficients equally.\n",
    "\n",
    "Furthermore, if you use gradient descent to compute linear regression, logistic regression, or neural networks, feature scaling can speed up the gradient descent. \n",
    "\n",
    "That’s because large-scale features will be modified first in gradient descent, since they seem to have more effect on the predictions. \n",
    "\n",
    "But these large-scale features may be modified excessively, ignoring the small-scale features that are also important. \n",
    "\n",
    "If the large-scale features are modified too strongly, they may “overshoot” the right values. E.g., if the coefficient is currently 2 but should be 1, it may “overshoot” to 0; then on the next step, it jumps back to 2 again. \n",
    "\n",
    "If the learning rate is too large (it takes steps that are too big), then it may never be satisfied that it has reached the right value. \n",
    "\n",
    "In that case, the small steps may be ignored.\n",
    "\n",
    "### Think About It\n",
    "- Why is feature scaling important to models’ performance? \n",
    "- Why does logistic regression require independent samples? \n",
    "- What is the difference between standardization and normalization? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2319d",
   "metadata": {},
   "source": [
    "## Week 4 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381350c1",
   "metadata": {},
   "source": [
    "Q1: Which of these is the standardized version of the array `[1, 2, 4]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c23fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.06904497 -0.26726124  1.33630621]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 4])\n",
    "standardized_x = (x - np.mean(x)) / np.std(x)\n",
    "print(standardized_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4341a",
   "metadata": {},
   "source": [
    "Which of these is the normalized version of the array `[1, 2, 4]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5e11d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.33333333 1.        ]\n"
     ]
    }
   ],
   "source": [
    "x_normalized = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "print(x_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be8028",
   "metadata": {},
   "source": [
    "Why would standardization be important with regression? \n",
    "- Option A - Gradient descent can be slow if some features are on a very different scale than others. \n",
    "- Option B - A linear regression will provide a better ﻿R squared﻿ value if the data is standardized. \n",
    "- Option C - Standardization can prevent overfitting. \n",
    "- Option D - Users cannot understand the data unless the features are standardized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e1cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
