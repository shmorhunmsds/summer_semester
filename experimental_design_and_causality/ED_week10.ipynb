{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e6fe95",
   "metadata": {},
   "source": [
    "# Week 10 Overview\n",
    "This week’s content introduces fixed effects models as a method for controlling unobserved confounders in grouped data. It explains the logic of isolating within-group variation by assigning a fixed intercept to each group, allowing the estimation of consistent treatment effects when group-level characteristics are unknown or unmeasured. The distinction between between-group and within-group variation is emphasized, along with practical estimation techniques such as demeaning and dummy variable encoding. The material also covers two-way fixed effects, where multiple grouping structures are present, and explores the interpretation of model coefficients and R² values. Extensions and alternatives are presented, including random effects models, which rely on distributional assumptions, and clustered standard errors, which address correlation in error terms. The week concludes with hierarchical random effects models that incorporate partially predictable group-specific variation based on observed characteristics.\n",
    "\n",
    "### Learning Objectives \n",
    "At the end of this week, you will be able to: \n",
    "- Apply fixed effects and two-way fixed effects models. \n",
    "- Understand random effects. \n",
    "- Review heteroskedasticity and nonlinear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beac097",
   "metadata": {},
   "source": [
    "## Topic Overview: Fixed Effects Models\n",
    "Fixed effects models provide a powerful strategy for controlling for unobserved, group-level influences in observational data. When data is organized into meaningful groups — such as individuals over time, hospitals, classrooms, or cities — there may be hidden characteristics that differ across groups and confound the relationship between a treatment and an outcome. Fixed effects address this by assigning a separate baseline level, or intercept, to each group, effectively removing the influence of any group-specific confounders that remain constant within that group. By focusing on variation within each group rather than differences between groups, fixed effects models allow for a cleaner estimate of how changes in a predictor variable are associated with changes in the outcome, assuming the effect is consistent across groups. This section introduces the logic behind fixed effects, explains the distinction between within-group and between-group variation, and illustrates the concept with intuitive examples.\n",
    "\n",
    "### Learning Objectives\n",
    "- Apply fixed effects and two-way fixed effects models. \n",
    "- Understand random effects. \n",
    "- Review heteroskedasticity and nonlinear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd7fdf",
   "metadata": {},
   "source": [
    "## 1.1 Lesson: Fixed Effect Models and Their Applications\n",
    "\n",
    "### Fixed Effects: Controlling for the Unseen in Regression\n",
    "Controlling for confounding in regression helps us to estimate causal effects. By controlling for observed confounders, variables we can measure that might influence both the independent and dependent variables. \n",
    "\n",
    "For example, if we were studying the effect of education on income, we might control for \n",
    "- age, \n",
    "- gender, and \n",
    "- work experience \n",
    "\n",
    "To isolate the effect of education. But what about unobserved confounders, characteristics we can't measure that might bias our estimates? This is where **fixed effects models** come in. \n",
    "\n",
    "**Fixed effects** \n",
    "- control for unobserved confounders that are constant within groups. \n",
    "- Even if we don't know exactly what unmeasured factors are influencing the outcome, we can neutralize them by controlling for group specific intercepts. \n",
    "\n",
    "This brings us to an interesting linguistic twist: In fixed effects models, the term **group** can refer to something as small as an individual person. In this context, an **individual can be treated as a \"group\"**. \n",
    "\n",
    "Let's start with an example where the group is actually an individual: \n",
    "- Suppose we're studying the relationship between a person's daily sleep and their mood. The model might look like: \n",
    "\n",
    "$$\\text{Mood}_{it} = \\beta_i + \\beta_1 \\cdot \\text{Sleep}_{it} + \\varepsilon_{it}$$ \n",
    "\n",
    "Here, \n",
    "- $t$ indexes days \n",
    "- $i$ indexes individuals. \n",
    "- $\\beta_i$ is the fixed effect for an individual. \n",
    "\n",
    "This setup controls for all time invariant characteristics of that person, such as: \n",
    "- Their genetics, \n",
    "- personality, \n",
    "- baseline stress level, \n",
    "- lifestyle, etc.\n",
    "\n",
    "We don't need to observe or measure those factors. We just need to account for the fact that person A is systematically different from person B. By including $\\beta_i$, we ensure that the variation used to estimate $\\beta_1$ comes from within person changes over time. So even though we call it a group level effect, in this case the group is just one person. \n",
    "\n",
    "Now consider a more traditional use of the word group. Suppose we're comparing average wages across cities and want to control for unobserved city specific factors like:\n",
    "\n",
    "**Average Wages Across Cities (Unobserved Confounders)**\n",
    "- economic climate, \n",
    "- cost of living or \n",
    "- local industry structure \n",
    "\n",
    "Then: \n",
    "$$\\text{wage}_{it} = \\beta_i + \\beta_1 \\cdot \\text{education}_{it} + \\varepsilon_{it}$$ \n",
    "\n",
    "Here, \n",
    "- $i$ indexes cities, \n",
    "- $t$ indexes individuals within cities, \n",
    "- $\\beta_i$ captures city specific unobservables \n",
    "\n",
    "Even if we don't know why wages are higher in San Francisco than in Omaha, by including city fixed effects $\\beta_i$, we control for whatever unmeasured city level characteristics might be influencing wages. \n",
    "\n",
    "In summary, \n",
    "- regression controls for **observed** variables. \n",
    "- Fixed effects control for **unobserved** time invariant variables as long as they are constant within a defined group. That group can be an individual. \n",
    "\n",
    "Repeated measures on a person or a more natural group, for example a city, a firm, a school. Fixed effects allow us to say we don't need to know what makes this group different. We just need to know that it is.\n",
    "\n",
    "### Additive Effects and Fixed Effects Regression\n",
    "In fixed effects models, we assume that group specific effects are additive. That means each group gets its own intercept beta I, which shifts the outcome up or down, but does not interact with the independent variable X. \n",
    "\n",
    "For example, recall the earlier model: \n",
    "\n",
    "$$\\text{Y}_{it} = \\beta_i + \\beta_1 \\cdot \\text{X}_{it} + \\varepsilon_{it}$$ \n",
    "\n",
    "Here,  \n",
    "- $\\beta_I$ is added, not multiplied. We're saying that each group $i$ starts at a different baseline $\\beta_i$. \n",
    "- But the effect of $X$ is the same across groups governed by $\\beta_1$. \n",
    "\n",
    "If we instead used a model like: \n",
    "\n",
    "$$Y_{it} = \\beta_i \\cdot X_{it} + \\varepsilon_{it}$$\n",
    "\n",
    "we'd be allowing the slope of X to vary by group. At this point, we might wonder what relationship is left between the groups. Are we just doing a separate linear\n",
    "relationship for each group? \n",
    "\n",
    "However, if we suspect a multiplicative relationship between the group effect and X, but the additive term is zero for all groups, we might use a **log-log** transformation: \n",
    "\n",
    "$$\\log(Y_{it}) = \\log(\\beta_i) + \\beta_1 \\cdot \\log(X_{it}) + \\varepsilon_{it}$$\n",
    "\n",
    "This transformation still fits within an additive model in log space, but corresponds to a multiplicative model in the original scale:\n",
    "\n",
    "$$Y_{it} = \\beta_i \\cdot X_{it}^{\\beta_1}$$ \n",
    "\n",
    "Our goal is then to identify the exponent which is now $\\beta_1$. \n",
    "\n",
    "To understand **fixed effects**, it's helpful to distinguish between two types of variation: \n",
    "- **Between group variation** is the difference between group means, \n",
    "    - for example, the average wage in New York versus Chicago. \n",
    "- **Within group variation** is the differences within a group over time or across individuals, like how a single person's wage changes as their education increases. \n",
    "\n",
    "\n",
    "Fixed effects models ignore between group variation and focus entirely on within group variation: \n",
    "- The idea is that if we want to know the effect of $X$, we estimate it by observing how changes in $X$ relate to changes in $Y$ within the same group. \n",
    "- After removing each group's baseline, one simple way to implement fixed effects is by: \n",
    "    - demeaning the data, \n",
    "    - subtracting each group's mean from its observations. \n",
    "    \n",
    "For example, suppose we have the following: \n",
    "\n",
    "**Group 1:** (X,Y) = (0,0) and (2,2) implies group mean = (1,1)\n",
    "\n",
    "**Group 2:** (X,Y) = (1, 1) and (3,5) implies group mean = (2,3)\n",
    "\n",
    "\n",
    "Now subtract the group means from each data point: \n",
    "\n",
    "**Group 1:** (X - mean, Y - mean) = (-1, -1) and (1,1)\n",
    "\n",
    "**Group 2:** (-1, -2) and (1, 2)\n",
    "\n",
    "These demeaned values reflect only the within group variation. When we run a regular regression on these adjusted values, we get an estimate of:\n",
    "\n",
    "\n",
    "$\\beta_1$ =  **The effect of X on Y** \n",
    "\n",
    "Within each group after accounting for any constant group level differences. Fixed effects models assume additive group specific effects. Each group has its own baseline or intercept, but the effect of $X$ the slope is shared. \n",
    "\n",
    "You can estimate this by demeaning or by subtracting the group mean from each observation and running a simple regression on the centered data. This lets you control for unobserved time invariant characteristics of the group, even if you don't know what those characteristics are \n",
    "\n",
    "#### One Way Fixed Effects:\n",
    "In **One-Way Fixed Effects** we control for unobserved time invariant characteristics of individuals or other units. For example, if we think each person has their own baseline level of income, regardless of their education or experience, we can Model: \n",
    "\n",
    "$$Y_{it} = \\beta_i + \\beta_1 \\cdot X_{it} + \\varepsilon_{it}$$ \n",
    "\n",
    "Alternatively, we can express this using dummy binary variables for each group, omitting one to avoid multicollinearity. \n",
    "\n",
    "Suppose we have four individuals, $A, B, C, D$. We might write: \n",
    "\n",
    "$Y_{it} = B_i + C_i + D_i + \\beta_1 \\cdot X_{it} + \\varepsilon_{it}$\n",
    "\n",
    "$B_i = 1$ if $i = B$, $0$ otherwise\n",
    "\n",
    "$C_i = 1$ if $i = C$, $0$ otherwise\n",
    "\n",
    "$D_i = 1$ if $i = D$, $0$ otherwise\n",
    "\n",
    "$A$ is the reference group which $i$s assumed if all three are zero.\n",
    "\n",
    "\n",
    "This is equivalent to assigning each individual a fixed intercept. It's just a different way of writing the fixed effects model. If you also believe there are unobserved time specific factors, for example Macroeconomic shocks that affect all individuals in a given year, you can include **time fixed effects** as well.\n",
    "\n",
    "#### Two-Way Fixed Effects Models\n",
    "Measures unobserve *time-specific* factors\n",
    "$$Y_{it} = \\alpha_t + \\beta_i + \\beta_1 \\cdot X_{it} + \\varepsilon_{it}$$ \n",
    "\n",
    "where, \n",
    "- $\\beta_i$ captures **individual-specific** effects, \n",
    "- $\\alpha_t$ captures **time-specific** effects. \n",
    "\n",
    "In panel regressions we often report multiple versions of $R^2$. All follow the general formula:$ R^2 = 1 - \\frac{RSS}{TSS}$, but the total sum of squares $TSS$ is defined differently depending on the type of $R^2$. \n",
    "\n",
    "Overall $R^2 \\mathbf{TSS}$ is calculated relative to the overall mean of $Y$, ignoring group structure. This tells you how much of the total variation in $Y$ across all observations is explained by the model. \n",
    "\n",
    "Within $R^2$, $\\mathbf{TSS}$ is calculated relative to the group specific means of $Y$. This reflects how much of the within group variation. (e.g., variation over time for a given individual or group) is explained by the model. \n",
    "\n",
    "In fixed effects models within $R^2$ is often more meaningful, because a model is designed to explain variation within each group, not between groups. \n",
    "\n",
    "\n",
    "For example, if we are demeaning:\n",
    "\n",
    "$Y_{it}^* = Y_{it} - \\bar{Y}_i$\n",
    "\n",
    "$X_{it}^* = X_{it} - \\bar{X}_i$\n",
    "\n",
    "if you demean each group's $$X$ and $Y$ values (i.e. subtract the group mean), then run a regression on the transformed data: \n",
    "\n",
    "$Y_{it}^* = \\beta_1 X_{it}^* + \\varepsilon_{it}$\n",
    "\n",
    "You are estimating the within group effect of $X$ on $Y$. \n",
    "\n",
    "In this case, $\\mathbf{TSS}$ for within $R^2$ is based on the variation of $Y$ from its group mean, not the overall mean.\n",
    "\n",
    "### Controlling for Unobserved Confounders \n",
    "Suppose we have data in $N$ **groups**: \n",
    "- We assume that each group has its own effect — a fixed effect — and that, apart from that effect, estimates are made using a linear function of treatment. \n",
    "- For example, \n",
    "    - Suppose that a hospital is a group, and its doctors are samples. \n",
    "    - The “treatment” is a doctor’s years of experience, and we want to see if experience influences patient outcomes. \n",
    "    - We think there may be a fixed effect due to the hospital (some hospitals have better outcomes) and a treatment effect (the doctor’s experience). \n",
    "- In another example, a child’s height is measured repeatedly over time. \n",
    "    - Then the child could be a “group,” and the treatment is “time.” \n",
    "    - Thus, the height could be assumed to grow linearly with time. (Confusingly, the child is an *individual* person but represents a *group* of observations over time. \n",
    "    - Each observation can be written as ($C, T$) — a child $C$ at time $T$. Thus, the word “individual” may sometimes be used in place of the word “group” if the group corresponds to an individual entity.) \n",
    "\n",
    "The ideal use of fixed effects occurs when we don’t know *which* confounders associated with the group may be important: \n",
    "- Thus, if we knew that the hospital’s impact on patient outcomes was related to the hospital’s reputation, its profitability, its size, or some other factor, we could control for those confounders. \n",
    "- If we don’t know any of that, we just assign a fixed effect to the hospital. For this to work, we need that variation within the group (the hospital) to be described by known parameter(s), i.e., the doctor’s experience. \n",
    "\n",
    "We assume that the effect is **additive:** \n",
    "- That is, the doctor’s patient outcomes can be computed as some base value for that hospital (the fixed effect) plus a linear function of the doctor’s experience. \n",
    "- What we don’t want is, say, a multiplier effect, where the doctor’s outcome is a base value multiplied by a linear function of the doctor’s experience. (Although, if we take the log of the patient outcome, we could capture a multiplicative effect.)\n",
    "\n",
    "### Between and Within Variation\n",
    "**“Between Variation”** is the difference between the means of each group. **“Within Variation”** is the difference between the samples within a group. \n",
    "- **Between Variation** compares the group means of $X$ and $Y$ — asking whether groups differ in their average $X$ and average $Y$. \n",
    "- **Within Variation** compares the specific $X$ and $Y$ values relative to their group means — asking whether people (or samples) within a group who have higher $X$ also tend to have higher $Y$. \n",
    "\n",
    "Calculating fixed effects requires considering *both* between and within variation. \n",
    "\n",
    "### Fixed Effects in Action\n",
    "The intent of **fixed effects** is essentially to remove the *“between variation”*, leaving only the “*within variation*.” \n",
    "\n",
    "Interestingly, the relationship between variables across groups can point in one direction (say, negative), while the pattern within each group might trend the opposite way (say, positive) once you remove group-level averages from both the independent and dependent variables. \n",
    "\n",
    "One way to picture this is to imagine shifting each group’s data so that its average value for both $X$ and $Y$ lines up at the origin:\n",
    "- Then, you could plot all the adjusted data points together on a single set of axes to visualize how $X$ and $Y$ move relative to each other within groups. \n",
    "- In this model, we’re assuming that the relationship between $X$ and $Y$ within each group follows a similar trend — aside from some random variation. \n",
    "- That is, within the group, $X = \\beta_i + \\beta_1Y + \\varepsilon$, where $\\beta_i$ depends on the group and $\\beta_1$ is the same for all groups.\n",
    "\n",
    "### Regression Estimators\n",
    "A fixed effects regression has the following equation, using $i$ to index groups:\n",
    "\n",
    "$Y_{it} = \\beta_i + \\beta_1X + \\varepsilon_i$\n",
    "\n",
    "or, using $t$ to index samples within each group:\n",
    "\n",
    "$Y_{it} = \\beta_i + \\beta_1X_{it} + \\varepsilon_{it}$\n",
    "\n",
    "Then $i$ is the group (the book uses “$i$” for “individual” because of the situation where the “group” is an individual person) and $t$ is the sample within each group (“$t$” for “time,” which is one approach to sampling). If $t$ is time, then it might make sense for $X_{it}$ and $X_{jt}$ to be at the same time. \n",
    "\n",
    "However, these samples don’t have to match. If $i$ and $j$ are different hospitals, then $X_{id}$ might be a doctor at hospital $i$, where there is no corresponding doctor (no “same doctor”) at hospital $j$. Then $\\beta_i$ is the intercept or fixed effect for each group $i$, while $\\beta_1$ is the within variation, which is meant to be identical across all groups and all samples. \n",
    "\n",
    "The idea is that the linear relationship for each group i has a different intercept, $beta_i$. \n",
    "\n",
    "There are two primary techniques for estimating the intercepts associated with each group $beta_i$:\n",
    "1. One method involves centering each individual’s data by subtracting their average $X$ and $Y$ values. After this transformation, the regression is performed on the deviations — that is, the adjusted $X$ and $Y$ values. This procedure effectively removes the group-specific baseline and is sometimes referred to as “absorbing the fixed effect.” For example, if our ($X$, $Y$) values are (0, 0) and (2, 2) in group 1 and (1, 1) and (3, 5) in group 2, then the means are (1, 1) for group 1 and (2, 3) for group 2. Subtracting these means gives (-1, -1) and (1, 1) in group 1 and (-1, -2) and (1, 2) in group 2. We can then perform a single regression over these four pairs to find $\\beta_1$.\n",
    "2. Another approach is to explicitly include a set of binary control variables — one for each group, leaving one out to avoid perfect multicollinearity. For example, if you have four categories labeled A through D, you’d introduce dummy variables for B, C, and D. Each takes the value 1 if the observation belongs to that group and 0 otherwise. This method becomes computationally intensive when there are many groups, as it results in a large number of additional variables.\n",
    "\n",
    "A coefficient $beta_1$ means that for a given group, each unit of increase in $X$ leads to a $\\beta_1$ increase in $Y$, under the assumption that this increase is “really” the same for all groups apart from noise.\n",
    "\n",
    "When we compute $R^2$ for “within” vs. “overall,” they use the same residual sum of squares (RSS), but they differ in how the total sum of squares (TSS) is calculated. For within, the TSS is taken as the variation of each group’s $Y$ values from their corresponding group means, whereas for overall, the TSS is taken as the variation of all $Y$ values from their common mean.\n",
    "\n",
    "Knowing that $R^2 = 1 - \\frac{RSS}{TSS}$, we can ascertain whether $R^2$ within or $R_{\\text{overall}}^2$ is larger. Thus, within $\\text{TSSwithin} < \\text{TSSoverall}$, so:\n",
    "\n",
    "$\\frac{\\text{RSS}}{\\text{TSSwithin}} > \\frac{\\text{RSS}}{\\text{TSSoverall}}$, and\n",
    "\n",
    "$1 - \\frac{\\text{RSS}}{\\text{TSSwithin}} < 1 - \\frac{\\text{RSS}}{\\text{TSSoverall}}$\n",
    "\n",
    "As for interpreting the group-specific intercepts (the $\\beta_i$), they represent how groups differ in their baseline $Y$ values when $X$ is held constant. For instance, if group A and group B had samples with identical $X$ values, but their intercepts differ ($\\beta_A$ and $\\beta_B$), the expected difference in $Y$ would be the gap between those intercepts. \n",
    "\n",
    "Lastly, it's important to note that groups where $X$ doesn't vary much provide limited information about the relationship between $X$ and $Y$. These groups contribute less to the estimate of $\\beta_1$, since only observations with substantial variation in $X$ help pin down the slope. If we want smaller-variation groups to have more influence, we can apply weights to balance their contribution in the analysis.\n",
    "\n",
    "### Multiple Sets of Fixed Effects \n",
    "It is possible to have multiple intersecting groups. Thus, imagine that each sample belongs to a group indexed by $i$ and a group indexed by $t$. For example, suppose $Y$ is the growth rate of each child ($i$) during a certain year of their life ($t$) as a function of the nutritive value of their food ($X_{it}$). We could imagine that there is a contribution that is particular to the child ($\\beta_i$) and a contribution that is particular to the year ($\\beta_t$) as well as a linear relationship with their food’s value (a coefficient $\\beta_1$). This would lead to the following regression:  \n",
    "\n",
    "$$Y_{it} = \\beta_i + \\beta_t + \\beta_1 X_{it} + \\varepsilon_{it}$$\n",
    "\n",
    "This is called **two-way fixed effects**. Neither of the groups has to be time; it could also be an individual ($i$) and a city ($c$). But you’ll need a sufficient number of individuals who have lived in two different cities; otherwise, you won’t be able to differentiate between the individual effect and the city effect. For example, suppose there are cities C1, C2, and C3 and individuals I1, I2, and I3, but only I3 has lived in C3, and I3 has never lived in any city other than C3. In that case, the fixed effect for that individual and city is $\\beta_{i3} + \\beta_{C3}$, and you can’t differentiate between the two betas. \n",
    "\n",
    "You could deal with two-way fixed effects using binary controls, which just requires twice as many variables as if you had one-way fixed effects. (The city controls plus the individual controls, in the case above.) \n",
    "\n",
    "We can also use “alternating projections,” which is a method specifically used to handle two-way (or more) fixed effects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155c560",
   "metadata": {},
   "source": [
    "## 1.2 Lesson: Extensions and Alternatives to Fixed Effects\n",
    "\n",
    "### Random Effects:\n",
    "In this model:  \n",
    "\n",
    "$Y_{it} = \\beta_i + \\beta_1X_{it} + \\varepsilon_{it}$  \n",
    "\n",
    "Random effects assumes that we know something about the $\\beta_i$’s before we start. We assume that they come from a known random distribution. If that’s the case, we can estimate them more precisely. For example, if $beta_i$ comes from a distribution with mean 0 and standard deviation 1, then we would assume that $\\beta_i = 10$ is highly implausible while $\\beta_i = 1$ is quite plausible. We require that the $\\beta_i$’s are independent of the $X_{it}$’s. For example, if $X_{it}$ is the GDP of country $i$ at time $t$, then suppose we know that one country has $X_{it} = 1, 2, 3$ and the other country has $X_{it} = 10, 11, 12$. In order for random effects to work, we want that $\\beta_i$ comes from the same distribution (the same mean and standard deviation) in either case. \n",
    "\n",
    "This criterion is usually not met, so in practice, random effects is useful only in an unlikely edge case.\n",
    "\n",
    "### Clustered Standard Errors\n",
    "What if the errors $\\varepsilon_{it}$ are not independent? \n",
    "\n",
    "For example, imagine that we are measuring people’s wage as a function of their education:\n",
    "\n",
    "$W_{it} = \\beta_i + \\beta_1, \\text{education}_{it} + \\varepsilon_{it}$\n",
    "\n",
    "Including $\\beta_i$ ensures that the mean error can be zero. \n",
    "\n",
    "But what if $\\varepsilon_{i1}$ and $\\varepsilon_{i2}$ are correlated for individual $i$ (that is, group $i$)? That is, if $\\varepsilon_{i1} = \\$ 100$, then $\\varepsilon_{i2}$ is more likely to be $90 or $110 than it is to be -$100. \n",
    "\n",
    "\n",
    "In that case, the errors correlate. Our estimate of $\\beta_1$ will be unbiased (just as likely to be too high as to be too low), assuming education is uncorrelated with the error. But the standard errors will be underestimated, which affects statistical inference. However, the standard errors will be wrong. If we see $100, $110, $90, we might assume that $\\varepsilon_i$ has a standard deviation around 10, when really it is more like 100.  \n",
    "\n",
    "Even if we have many samples, correlated errors across time will cause problems. For example, suppose $\\varepsilon_{it}$ looks like -$100, -$92, -$77, ..., $94, $100, gradually growing over time. Then we don’t really have 20 or so independent samples; we more or less just have two, because the other errors would seem to be determined by the -$100 and the $100. \n",
    "\n",
    "This makes our standard errors wrong because the standard error formula contains $\\frac{1}{\\sqrt{N}}$ , which will be $\\frac{1}{\\sqrt{20}}$ when it should be $\\frac{1}{\\sqrt{2}}$. \n",
    "\n",
    "The idea behind clustered standard errors is to obtain the correct standard error value even though the error epsilon is correlated across time. (Or otherwise correlated across samples — e.g., classrooms, firms, or geographic regions.) \n",
    "\n",
    "Nonlinear models (i.e. $Y = F (\\beta_ + \\beta_1X)$) are tricky to do with fixed effects. We could either: \n",
    "1. Use a linear model instead, even if it isn’t the best possible fit. \n",
    "2. Use a nonlinear model that’s suitable for fixed effects. \n",
    "\n",
    "### Advanced Random Effects\n",
    "We can use hierarchical random effects, which means that the individual effects ($\\beta_i$) are not completely random but have a structure. In this approach, $\\beta_i$ is treated as a variable that can be modeled, and we assume that it is a function of some predictors $Z_i$. Specifically, we model:\n",
    "\n",
    "$Y_{it} = \\beta_i + \\beta_1X_{it} + \\varepsilon_i$\n",
    "\n",
    "where\n",
    "\n",
    "$\\beta_i = \\beta_0 + \\gamma_1Z_i + \\mu_i$\n",
    "\n",
    "Here, $\\beta_i$ consists of three components:\n",
    "1. A fixed intercept(\\beta_0)\n",
    "2. A predictable component that depends on $Z_i$ (for example, the log of population)\n",
    "3. A random component($\\mu_i$) that captures unobserved individual-specific variation.\n",
    "\n",
    "\n",
    "In essence, we are saying that $\\beta_i$ has a systematic structure where part of it can be predicted based on observable values (like the population of a city), while the rest of it remains random. \n",
    "\n",
    "For example, \n",
    "- If $i$ represents a city, and\n",
    "- $Z_i$ is the log of its population,\n",
    "- We interpret $\\beta_i$  as being driven by two factors:\n",
    "    - one that is predictable based on population,\n",
    "    - another that is random (reflecting unobserved city-specific influences)\n",
    "- this approach allows us to assume that the $\\beta_i$'s are not completely independent, but rather have some underlying order or rationale. \n",
    "\n",
    "To estimate this model, we used correlated random effects or hierarchical linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d955c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2a25f",
   "metadata": {},
   "source": [
    "### Knowledge Check: Fixed Effects\n",
    "1. Suppose there are two groups, which have: \n",
    "- A: (X = 2, Y = 3), (X = 5, Y = 4)\n",
    "- B: (X = 5, Y = 4), (X = 2, Y = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6651cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group A Mean: [3.5 3.5]\n",
      "Group B Mean: [3.5 3.5]\n",
      "Group A Variance: [2.25 0.25]\n",
      "Group B Variance: [2.25 0.25]\n",
      "Between Variation: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 3], [5, 4]])\n",
    "B = np.array([[5, 4], [2, 3]])\n",
    "\n",
    "# Calculate the means of X and Y for each group\n",
    "mean_A = np.mean(A, axis=0)\n",
    "mean_B = np.mean(B, axis=0)\n",
    "\n",
    "print(\"Group A Mean:\", mean_A)\n",
    "print(\"Group B Mean:\", mean_B)\n",
    "\n",
    "# calculate the variation of X and Y for each group\n",
    "var_A = np.var(A, axis=0)\n",
    "var_B = np.var(B, axis=0)\n",
    "\n",
    "print(\"Group A Variance:\", var_A)\n",
    "print(\"Group B Variance:\", var_B)\n",
    "\n",
    "# calculate between variation between the two groups\n",
    "between_variation = np.mean(A, axis=0) - np.mean(B, axis=0)\n",
    "print(\"Between Variation:\", between_variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075997f",
   "metadata": {},
   "source": [
    "The between variations are zero because X and Y are the same on average for the two groups. Within groups, X varies more than Y in both groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09276880",
   "metadata": {},
   "source": [
    "2. Using the same two groups from (1), if we recenter the groups so that the origin of each group is (shifted to 0, 0), the result for group A is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef6bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centered Group A: [[-1.5 -0.5]\n",
      " [ 1.5  0.5]]\n",
      "Centered Group B: [[ 1.5  0.5]\n",
      " [-1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "# recenter the groups so that the origin of each group is (0, 0)\n",
    "A_centered = A - mean_A\n",
    "B_centered = B - mean_B\n",
    "print(\"Centered Group A:\", A_centered)\n",
    "print(\"Centered Group B:\", B_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcbb67",
   "metadata": {},
   "source": [
    "The origin (3.5, 3.5) has to be subtracted from A. (And also from B.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543f8ab",
   "metadata": {},
   "source": [
    "We can use either, but if we use both, we are duplicating information: X_A being 1 is exactly the same as X_B being 0, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a759b4",
   "metadata": {},
   "source": [
    "3. Using the same two groups from 1, if we instead introduced dummy variables we might use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ed992",
   "metadata": {},
   "source": [
    "1. Understanding the Concepts:\n",
    "- Within-group variation:\n",
    "    - Measures how spread out the data points are within each individual set. A higher within-group variation indicates more diversity within a set. For example, if you have test scores for two classes, a high within-group variation in one class means some students did very well, while others did poorly. \n",
    "- Between-group variation:\n",
    "    - Measures how different the average values (means) of the sets are from each other. A higher between-group variation suggests the sets are more distinct. \n",
    "2. Statistical Analysis:\n",
    "- ANOVA (Analysis of Variance):\n",
    "    - This is a powerful statistical method used to compare the means of two or more groups by analyzing variance. ANOVA breaks down the total variance in the data into components attributable to different sources, namely, between-group variation and within-group variation. It uses an F-test to determine if the between-group variation is significantly larger than the within-group variation. \n",
    "- T-tests:\n",
    "    - T-tests are used to compare the means of two groups. They help determine if the difference between the means is statistically significant or likely due to chance. T-tests assume a null hypothesis (that the means are equal) and calculate a p-value to assess the probability of observing the data if the null hypothesis were true. \n",
    "- Other Tests:\n",
    "    - Depending on the data and research question, other tests like chi-square tests or z-tests (for large sample sizes) might be appropriate. \n",
    "3. Determining Significance:\n",
    "- P-value:\n",
    "    - In statistical tests, the p-value represents the probability of observing the results (or more extreme results) if the null hypothesis is true. A small p-value (typically below 0.05) indicates strong evidence against the null hypothesis, suggesting that the observed differences are statistically significant. \n",
    "- F-statistic:\n",
    "    - In ANOVA, the F-statistic represents the ratio of between-group variance to within-group variance. A large F-statistic suggests that the between-group variation is greater than the within-group variation, increasing the likelihood of rejecting the null hypothesis. \n",
    "4. Example: Imagine comparing the effectiveness of two different teaching methods. You collect test scores from students using each method. To determine if the methods differ significantly, you would: \n",
    "- Calculate the mean score for each class (within-group variation).\n",
    "- Calculate the overall mean score for all students.\n",
    "- Analyze the variance within each class (how spread out the scores are within each class).\n",
    "- Compare the difference between the class means (between-group variation) to the variability within each class.\n",
    "- If the difference between the class means is large relative to the variability within each class (and the p-value is small), you can conclude that the teaching methods have a statistically significant impact on student performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e447c2b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
