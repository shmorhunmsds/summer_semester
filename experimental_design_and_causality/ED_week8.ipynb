{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aac1961",
   "metadata": {},
   "source": [
    "# Week 8 Overview\n",
    "Matching is a powerful alternative to regression for estimating treatment effects when linear assumptions may not hold or when the relationship between variables is complex. This content introduces the concept of matching as a way to “close back doors” by pairing treated and untreated units with similar covariates, enabling more accurate estimation of causal effects. You’ll explore different matching techniques, including distance-based matching, propensity score matching, inverse probability weighting, and kernel-based weighting. The content also covers how to match using one or multiple variables, how to assess balance and common support, and how to handle practical challenges like bias-variance tradeoffs and the curse of dimensionality. By the end, you’ll understand how matching can be used on its own or combined with regression to support valid causal inference. \n",
    "\n",
    "## Learning Objectives \n",
    "At the end of this week, you will be able to: \n",
    "- Explain matching and its application to average treatment effect (ATE), ATT (average treatment effect on the treated), and ATUT (average treatment effect on the untreated) \n",
    "- Perform inverse probability weighting and propensity score matching  \n",
    "- Explain matching variants such as the Mahalanobis distance and the Epanechnikov kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b2422",
   "metadata": {},
   "source": [
    "## Topic Overview: Matching Fundamentals and Strategies\n",
    "This section introduces matching as a flexible alternative to regression for closing back doors when the relationship between variables is complex or nonlinear. \n",
    "\n",
    "Instead of modeling the functional form directly, **matching** pairs treated and untreated observations with similar values of a confounding variable, $Z$. \n",
    "\n",
    "This allows estimation of treatment effects, like ATT, even without knowing the true form of the relationship between treatment and outcome. \n",
    "\n",
    "The section also covers **weighted matching** as a way to account for distributional imbalances and introduces key concepts like distance matching and propensity score matching, which help formalize similarity between observations using one or more covariates. \n",
    "\n",
    "### Learning Objectives \n",
    "- Explain matching and its application to ATE, ATT, and ATUT \n",
    "- Perform inverse probability weighting and propensity score matching "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b52cd",
   "metadata": {},
   "source": [
    "### 1.1 Lesson: Another Way to Close Back Doors\n",
    "What if the relationship between treatment and outcome isn’t something we can easily model with a straight line? In this video, we’ll explore how **matching** can help us estimate treatment effects—even when the relationship between variables is complex, messy, or nonlinear. \n",
    "\n",
    "#### Matching\n",
    "With regression, we close back doors by finding a linear relationship between the outcome and the treatment, as well as with any confounders. But what if there isn’t a linear relationship, and we’re not sure how to model the situation? \n",
    "\n",
    "For example, what if:\n",
    "\n",
    "$$ Y = \\frac{2X + X^2}{Z} + \\varepsilon(X, Z) $$\n",
    "\n",
    "Then, we *could* model this linearly by including the term $\\frac{X^2}{Z}$  as an additional covariate, but what if we don’t think to do this? It’s a pretty weird relationship — we’re unlikely to think of it unless our domain knowledge hints of it:\n",
    "\n",
    "One solution would be matching: \n",
    "\n",
    "1. For each item with $X = 1$, we note its $Z$ value.\n",
    "2. Then we find another item with approximately the same $Z$ value, and match them. \n",
    "\n",
    "Suppose the $X = 0$ values have $Z = 1, 2$, while the $X = 1$ values have $Z = 1, 2, 3$. We throw out the $Z = 3$ item (it doesn't match), and we get: \n",
    "\n",
    "$Y(X = 0, Z = 1) \\; \\& = 0$\n",
    "\n",
    "$Y(X = 0, Z = 2) \\; \\& = 0$\n",
    "\n",
    "$Y(X = 1, Z = 1) \\; \\& = 3$\n",
    "\n",
    "$Y(X = 1, Z = 1) \\; \\& = 2.5$\n",
    "\n",
    "In this scenario the estimated effect is the difference between the average $X = 1$ and $X = 0$ values, or \n",
    "\n",
    "$$  \\frac{ (\\frac{3. + 2.5}{2} - \\frac{0}{2} )}{2} \\; = \\; 1.375$$\n",
    "\n",
    "This is the average treatment effect on the treated, since we are including counterfactuals (matches) for all treated items, but we are not including counterfactuals for all untreated items ($Z = 3$ has no counterfactual). Matching works even though we do not know the true relationship between $X$, $Y$, and $Z$. All we need to know is that $Z$ is the only confounder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960de87",
   "metadata": {},
   "source": [
    "### 1.2 Lesson: Weighted Averages\n",
    "We can also match by **weighting** \n",
    "\n",
    "In this case, if the untreated group has $Z = 1, 2, 3$ while the treated group has $Z = 1, 2$ \n",
    "\n",
    "Then we could weight the untreated values according to how close they are to the treated values: \n",
    "- For the untreated items, maybe the $Z = 1$ item would have a weight of 0.4, \n",
    "- $Z = 2$ has $0.4$, and \n",
    "- $Z = 3$ has 0.2 because $Z = 3$ is farther away from the treated $Z$ values. \n",
    "\n",
    "The exact weight is somewhat arbitrary: \n",
    "- Of the untreated items, $Z = 3$ should get a lower weight than $Z = 2$ because it’s farther from the treatment samples’ $Z$ values, but what weight, exactly, should it get?  \n",
    "\n",
    "There are many ways of calculating this, but in the end, it’s up to you. There is no one right answer. \n",
    "- For the treated items\" $Z = 1$ and $Z = 2$ both get an equal weight of $0.5$ \n",
    "- Then, to compute the effect, we weight each $Y$ value by the associated weight based on the $Z$ value. \n",
    "\n",
    "Another example could be if $Z$ is categorical: \n",
    "\n",
    "There are men and women. \n",
    "- If there are more men in the untreated group than in the treated group, we should downweight the untreated men (say, before computing the mean of ﻿Y﻿ or other statistic) according to the ratio of the count.\n",
    "- If there are also more women in the untreated group, we would downweight the untreated women, too, but likely by a different amount. \n",
    "- if the treated group has 8 men and 2 women, while the untreated group has 4 men and 6 women, then we could compute the ATT by weighting the untreated group so that men get a total weight of 0.8 and women 0.2. - This would involve weighting men at 0.2 each and women at 0.0333 each. The treated group would weight everyone at 0.1. In this way, we effectively have one counterfactual for each man and woman in the treated group. \n",
    "\n",
    "Taken together, these weights would allow us to compute the mean or other statistic of the untreated group as if it had the same distribution of men and women as the treated group. \n",
    "\n",
    "#### A Single Matching Variable\n",
    "As noted above, there’s a significant problem with trying to match the “nearest” $Z$ value between treated and untreated groups: \n",
    "- We need to know which $Z$ value is closest (and there may be multiple ways of measuring that). \n",
    "- When we are weighting, we need to know what weights to assign based on how close the $Z$ value is to other $Z$ values.\n",
    "\n",
    "The simplest approach is **distance matching:**\n",
    "- Distance Matching matches east treatment item to the closest untreated item(s) in distance, according ot the proximity of the matching variables (i.e., confounders like $Z$).\n",
    "- If there is one matching variable only, this is easy; but if there are several matching variables, we have to define a distance between samples where there are multiple covariates ($Z_1, Z_2, Z_3,$ etc.)\n",
    "- One way would be to standardize them, then take the euclidian distance, like $ \\sqrt{(Z_{1,T} - Z_{1, U})^2 + (Z_{2,T} - Z_{2, U})^2 + (Z_{3,T} - Z_{3, U})^2}$.\n",
    "    - Standardization is important because if the difference for $Z_1$ are on the order of 100, but for $Z_2$they are on the order of 1, then the $Z_1$ differences will completely outweight the $Z_2$ differences.\n",
    "    - This is especially true with Euclidean distances!, $\\sqrt{100^2 + 1^2}$  is $100.005$ ; the $1$ doesn't really matter because of the squaring.\n",
    "\n",
    "Perhaps the most common approach is **propensity score matching:**\n",
    "- In propensity score matching observations are similar if they were equally likely to be treated. \n",
    "- For example: \n",
    "    - if the matching variable is income, and the likelihood of treatment is normally distributed with mean $50,000, then the values $40,000 and $60,000 are “similar” because they are equally likely to be treated. - So, we can match a treated variable with income $40,000 to an untreated variable with $40,000 or with $60,000. This works to close the back door, although this may not be obvious. \n",
    "    \n",
    "To see why, suppose we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f88db34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Income</th>\n",
       "      <th>Target value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Treated</td>\n",
       "      <td>40000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Treated</td>\n",
       "      <td>50000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Treated</td>\n",
       "      <td>50000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Treated</td>\n",
       "      <td>60000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Untreated</td>\n",
       "      <td>40000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Group  Income  Target value\n",
       "0    Treated   40000            12\n",
       "1    Treated   50000            18\n",
       "2    Treated   50000            18\n",
       "3    Treated   60000             0\n",
       "4  Untreated   40000            12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dict = {\n",
    "    'Group' : ['Treated', 'Treated', 'Treated', 'Treated', 'Untreated', 'Untreated', 'Untreated', 'Untreated','Untreated', 'Untreated',],\n",
    "    'Income' : [40000, 50000, 50000, 60000, 40000, 40000, 50000, 50000, 60000, 60000],\n",
    "    'Target value' : [12, 18, 18, 0, 12, 12, 18, 18, 0, 0]\n",
    "    }\n",
    "df = pd.DataFrame(dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ab276",
   "metadata": {},
   "source": [
    "Then, taking the overall means results in the treated target value mean=12 and untreated=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee452e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Target value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Treated</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Untreated</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Income  Target value\n",
       "Group                           \n",
       "Treated    50000.0          12.0\n",
       "Untreated  50000.0          10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Group').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90843f42",
   "metadata": {},
   "source": [
    "But the income is a *confounder*; the two groups are actually the same when controlling for income. (This must be the case: the incomes’ targets match exactly across both groups.) \n",
    "\n",
    "If we match incomes using distance matching, for distance = 0, the untreated group gets 12, 18, 18, 0, and is the same target mean as the control group. \n",
    "\n",
    "This would involve the treated item with income $40,000 matching to the average of the two untreated items, with income $40,000 (all three of these have target = 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc9da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data means by treatment status:\n",
      "Group\n",
      "Treated      12.0\n",
      "Untreated    10.0\n",
      "Name: Target value, dtype: float64\n",
      "\n",
      "Matched data means:\n",
      "Treated mean: 12.0\n",
      "Untreated (matched) mean: 12.0\n",
      "Treatment effect: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/nl2zz_ln4d55dcscl_vvjglh0000gn/T/ipykernel_79623/373636855.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  matched_untreated = pd.concat([matched_untreated, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with numeric treatment indicator\n",
    "df_numeric = df.copy()\n",
    "df_numeric['Treatment'] = (df_numeric['Group'] == 'Treated').astype(int)\n",
    "\n",
    "# Separate the data into treated and untreated groups\n",
    "treated = df_numeric[df_numeric['Treatment'] == 1]\n",
    "untreated = df_numeric[df_numeric['Treatment'] == 0]\n",
    "\n",
    "# Create a new dataframe for matched pairs\n",
    "matched_treated = treated.copy()\n",
    "matched_untreated = pd.DataFrame(columns=df_numeric.columns)\n",
    "\n",
    "# For each treated observation, find matching untreated observations\n",
    "for _, treated_row in treated.iterrows():\n",
    "    # Find untreated observations with the same income (distance = 0)\n",
    "    matches = untreated[untreated['Income'] == treated_row['Income']]\n",
    "    \n",
    "    # If we found matches, add them to our matched dataset\n",
    "    if not matches.empty:\n",
    "        # Calculate the average of the matched untreated observations\n",
    "        avg_target = matches['Target value'].mean()\n",
    "        avg_income = matches['Income'].mean()\n",
    "        \n",
    "        # Create a new row for the matched untreated observation\n",
    "        new_row = pd.DataFrame({\n",
    "            'Group': ['Untreated (Matched)'],\n",
    "            'Income': [avg_income],\n",
    "            'Target value': [avg_target],\n",
    "            'Treatment': [0]\n",
    "        })\n",
    "        \n",
    "        # Add this match to our matched untreated data\n",
    "        matched_untreated = pd.concat([matched_untreated, new_row], ignore_index=True)\n",
    "\n",
    "# Combine the matched treated and untreated data\n",
    "matched_data = pd.concat([matched_treated, matched_untreated], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Original data means by treatment status:\")\n",
    "print(df.groupby('Group')['Target value'].mean())\n",
    "\n",
    "print(\"\\nMatched data means:\")\n",
    "print(\"Treated mean:\", matched_treated['Target value'].mean())\n",
    "print(\"Untreated (matched) mean:\", matched_untreated['Target value'].mean())\n",
    "print(\"Treatment effect:\", matched_treated['Target value'].mean() - matched_untreated['Target value'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e7e8b",
   "metadata": {},
   "source": [
    "But what about propensity score matching? \n",
    "\n",
    "This would mean that we can randomly cross-match untreated-$40,000 and untreated-$60,000 to treated-$40,000 and treated-$60,000 if we like because they have the same propensity to be treated (one-third of each are treated, compared with one-half for $50,000). \n",
    "\n",
    "This will produce the same target value average in the untreated matched group as for distance matching (in this case, 12). \n",
    "\n",
    "The propensity score approach involves treated-$40,000 matching to untreated-$40,000 or untreated-$60,000 and untreated-$60,000 to untreated-$40,000 or untreated-$60,000. In other words, the two untreated values are equally likely to be chosen. \n",
    "\n",
    "The distance approach involves treated-$40,000 matching to untreated-$40,000 and treated-$60,000 matching to untreated-$60,000. So again, the two untreated values are equally likely to be chosen (they are chosen once each). \n",
    "\n",
    "Then, having a 50% likelihood of picking $40,000 or $60,000 for the match gives the same average target value as always picking exactly one $40,000 and one $60,000. \n",
    "\n",
    "An alternative perspective is that the confounder (income) influences the propensity score (which is 33% for the high and low income, 50% for the medium one), which in turn influences the treatment. If we think of this in terms of directed acyclic graphs, we’ll see that the path through income toward the outcome (﻿Y﻿) must pass through the propensity score. Therefore, controlling for the propensity score suffices to control for this path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035588ae",
   "metadata": {},
   "source": [
    "## Topic 2: Practical Considerations and Advanced Techniques\n",
    "This section explains how to calculate propensity scores and use them effectively in matching and weighting strategies for causal inference. \n",
    "- You'll learn how logistic regression or binning can estimate treatment probabilities and explore practical decisions like how many matches to select, whether to match with or without replacement, and how to assign weights based on distance using kernels like Epanechnikov. \n",
    "- The section also introduces inverse probability weighting (IPW), considerations for caliper selection, matching in high-dimensional spaces, and advanced techniques like Mahalanobis distance, coarsened exact matching, and entropy balancing. \n",
    "- Finally, you'll see how to validate your propensity score models through stratification tests and how machine learning can enhance score estimation. \n",
    "\n",
    "### Learning Objectives \n",
    "- Perform inverse probability weighting and propensity score matching  \n",
    "- Explain matching variants such as the Mahalanobis distance and the Epanechnikov kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e38978",
   "metadata": {},
   "source": [
    "### 2.1 Lesson: Propensity Scores and Matching Strategies\n",
    "When people receive a treatment, it’s often not random — it’s influenced by their characteristics. In this video, you’ll learn how propensity scores help us account for those differences and how we can use them — along with a method called inverse probability weighting — to estimate causal effects more accurately.\n",
    "\n",
    "#### How to Calculate Propensity Scores\n",
    "With one covariate (such as income in the example above), we’d often just match on the value of the covariate itself. But if we wanted to use propensity score, we could compute it in one of two ways:\n",
    "- Perform logistic regression to model the probability of treatement. Above, we'd want to fit a quadratic function where:\n",
    "$$\\text{Prob} (40000) = \\frac{1}{3}, \\text{Prob} (50000) = \\frac{1}{2}, \\text{Prob} (60000) = \\frac{1}{3}$$\n",
    "- It has to be quadratic, including a $Z^2$ term, because we want it to be small for both high and low incomes and larger for the middle incomes.\n",
    "- Bin the data(in this case, perhaps into bins of 10,000) and compute the probability of treatment in each bin.\n",
    "\n",
    "#### Selecting Matches or Constructing a Matched Weighted Sample?  \n",
    "Selecting matches means selecting one or more untreated observations for each treated observation (or vice-versa). \n",
    "\n",
    "In a matched weighted sample, each untreated observation is assigned a numerical “weight” that determines how important it is. \n",
    "\n",
    "The “selecting matches” effect can vary more with small changes to the dataset as the matched observations can be “in” or “out” according to small differences in $Z$ values. \n",
    "\n",
    "Suppose a treated item has $Z = 0.55$, and the two nearest untreated items have $Z = 1$ and $Z = 0$. \n",
    "\n",
    "This treated item is closer to $Z = 1$ than to $Z = 0$. But a small change to $Z = 0.45$ can make it closer to $Z = 0$.\n",
    "\n",
    "#### If We’re Selecting Matches, How Many? \n",
    "\n",
    "Suppose we compute the ATT so that we find untreated matches (counterfactuals) for each treated item. There might be multiple good matches for each treated item. What do we do in that case? There are several options. \n",
    "\n",
    "1. Pick the best match. \n",
    "2. Pick the top k best matches (k-nearest-neighbor matching). We can then average their $Y$ values in order to find the effect. \n",
    "3. Pick all the matches within a given distance (radius matching). We can find all untreated $Z$ values within a radius of a given treated $Z$ value. Again, we average their $Y$ values. Radius matching can assign a varying number of matches to each treated item, so we’d want to weight these matches so that the effective number of matches to each treated item is 1. If one treated item has two matches and another has three, we’d weight the former by 0.5 each and the latter by 0.333 each. \n",
    "\n",
    "We can also match “with replacement,” meaning that a given untreated observation can match to multiple treated observations, or “without replacement,” meaning that each untreated observation can only match to one treated observation. \n",
    "\n",
    "One suggestion: If your untreated group contains many more observations than your treated group, it will make more sense to assign multiple untreated observations to each treated one. If your untreated group contains about the same number of observations as your treated group, you might not have enough observations to do that. \n",
    "\n",
    "#### If We’re Constructing a Matched Weighted Sample, How Will Weights Decay with Distance?\n",
    "In an example earlier, I said that the farther-away $Z = 3$ value would get a weight of $0.2$, while the closer ones got a weight of $0.4$ each. That was just a random guess — is there any more systematic way to assign weights based on distance?\n",
    "\n",
    "One approach is called the **Epanechnikov kernel**, which assigns a weight given the distance $x:$\n",
    "\n",
    "$K (x) \\; = \\; \\frac{3}{4} (1 - x^2)$\n",
    "\n",
    "In this case, since $Z = 3$ was a distance of 1 away from $Z = 2$, we'd have:\n",
    "\n",
    "$K(3 - 2) \\; = \\; \\frac{3}{4} (1 - (3-2)^2)$\n",
    "\n",
    "$ = \\; \\frac{3}{4} (1 - 1) \\; = \\; 0$ (So the $Z = 3$ value would be too far away to be included). \n",
    "\n",
    "If it were $Z = 2.5$ instead, we'd have $K(2.5 - 2) \\; = \\; \\frac{3}{4} (1 - (0.5)^2) \\; = \\; 0.5625$ as its weight. We might want to rescale the kernel if our typical distances are too large for too small for this kernel to be meaningful. \n",
    "\n",
    "The Epanechnikov kernel is similar to doing radius matching with a radius of 1, excep that the weight increases continuously up to this radius rather than assigning a disrete value of either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b249554",
   "metadata": {},
   "source": [
    "### 2.2 Lesson: Advanced Matching and Weighting Techniques, Part I\n",
    "\n",
    "#### Inverse Probability Weighting (IPW)\n",
    "This approach is used with propensity scores. \n",
    "- The weight for a given observation is equal to one divided by its probability. \n",
    "- For a treated observation, we use the probability that it would be treated. \n",
    "- For an untreated observation, we use the probability that it would be untreated. \n",
    "- This probability could be calculated using either (a) logistic regression based on one or more $Z$ values or (b) if $Z$ is categorical, we can simply count up the treatment probability for the given $Z$.\n",
    "\n",
    "**\"Treatment value”** means **“treated”** or **“untreated\":** \n",
    "- In other words, if items with income 40,000 have a propensity score of 33% (one-third of them are treated, and two-thirds are untreated) \n",
    "- Then they are weighted by $\\frac{1}{0.33} = 3$ in the treatment group, and $\\frac{1}{0.67} = 1.5$ in the control group.\n",
    "-  The result is that the total weight in the treatment and untreated group is, on average, the same (it is two times the number of items).\n",
    "- That's because for (on average) $Np$ items in *treatment* and $N(1 - p)$ items in the untreated grup, the total weight is $N \\cdot \\frac{p}{p} = N$ in treatement, and $N \\frac {1 - p}{1 - p} = N$ in the untreated group. \n",
    "\n",
    "In effect, if we have $N$ items of a certain type \\(around income \\$40,000, or having a similar propensity score to that\\), then inverse probability weighting assigns weights so that the total weight of these items is $2N$ , i.e., the average weight of each item is 2. \n",
    "\n",
    "What’s meant here is not that the items’ weights must total exactly $2N$ but that if:\n",
    "- (1) a number of samples were taken from the population and \n",
    "- (2) the model for computing the propensity is well-specified, with the correct formula, then on average, the total will be $2N$. \n",
    "\n",
    "\n",
    "\n",
    "This means that we are computing the average treatment effect (each item counts for 2, whether it is treated or untreated) instead of the average treatment effect on the treated (each treated item counts for a fixed amount, and untreated items count according to their treated match).\n",
    "\n",
    "Let’s consider the example above. If items with income 50,000 have a propensity score of 50%, they are weighted by $\\frac{1}{0.5} = 2$ in the treatment group and $\\frac{1}{1 - 0.5} = 2$ in the control group. In the above example, applying IPW to each sample, this results in a $Y$ value of \n",
    "\n",
    "$$\\frac {12 \\times 3 + 18 \\times 2 + 18 \\times 2 + 0 \\times 3}{10} = 10.8$$\n",
    "\n",
    "The same $Y$ value for treated and untreated.\n",
    "\n",
    "Let’s consider this special case where the treated and untreated items have the same target value given the same propensity score. There are $N_T$ of a given matching value in treated, which all have target value $T$ , and $N_U% in untreated, which also have target value ﻿T﻿ , then (assuming the statistical distribution of the data exactly reflects the \n",
    "\n",
    "\n",
    "#### What Is the Worst Acceptable Match?\n",
    "What if one treated observation is totally different from all untreated observations? \n",
    "- We define a “caliper” or “bandwidth,” which is a number of standard deviations (either of the propensity score or of the matched value, whichever you are using to match; in the latter case, I am assuming that the matched covariate is a single number ﻿Z﻿ so that it has a single standard deviation). \n",
    "- You can use the Epanechnikov kernel, which automatically has a caliper of 1. \n",
    "- You can use exact matching, which automatically has a caliper of 0. (This is unsuitable for continuous data values that rarely or never take on the same value.) \n",
    "- You can use coarsened exact matching, where you bin the data and then require that the two matches (treated and untreated) be in the same bin. (This is suitable for continuous data values.) \n",
    "\n",
    "Recall the **bias-variance tradeoff:** \n",
    "- High bias means that our prediction is systematically wrong — even if we took an infinite number of samples and averaged the results, it would still be wrong. \n",
    "- High variance means that our prediction varies dramatically when we take different samples. The wider the bandwidth, the more bias because the $Y$ values in the large bin may differ systematically from each other. \n",
    "- However, the wider the bandwidth, the less variance because we sample more possible matches. In other words: \n",
    "    - if we only take a single $Y$ value from a small bin, it should be correct on average but might be wrong by chance. \n",
    "    - If we take many matching $Y$ values from a large bin, the average will be stable, but that average might differ systematically from the correct value. \n",
    "    - Another way of putting it is that if the matches are at a distance of 1, 2, 3, 4, and 5 away (in terms of the matching value), then using just the distance 1 value has low bias (because we’re using the best estimator available) but high variance (we’d need to take an average of multiple estimates to reduce the variance.) \n",
    "\n",
    "The bias-variance tradeoff is generally important when performing matching! \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
