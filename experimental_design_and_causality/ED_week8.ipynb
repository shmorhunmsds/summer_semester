{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aac1961",
   "metadata": {},
   "source": [
    "# Week 8 Overview\n",
    "Matching is a powerful alternative to regression for estimating treatment effects when linear assumptions may not hold or when the relationship between variables is complex. This content introduces the concept of matching as a way to “close back doors” by pairing treated and untreated units with similar covariates, enabling more accurate estimation of causal effects. You’ll explore different matching techniques, including distance-based matching, propensity score matching, inverse probability weighting, and kernel-based weighting. The content also covers how to match using one or multiple variables, how to assess balance and common support, and how to handle practical challenges like bias-variance tradeoffs and the curse of dimensionality. By the end, you’ll understand how matching can be used on its own or combined with regression to support valid causal inference. \n",
    "\n",
    "## Learning Objectives \n",
    "At the end of this week, you will be able to: \n",
    "- Explain matching and its application to average treatment effect (ATE), ATT (average treatment effect on the treated), and ATUT (average treatment effect on the untreated) \n",
    "- Perform inverse probability weighting and propensity score matching  \n",
    "- Explain matching variants such as the Mahalanobis distance and the Epanechnikov kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b2422",
   "metadata": {},
   "source": [
    "## Topic Overview: Matching Fundamentals and Strategies\n",
    "This section introduces matching as a flexible alternative to regression for closing back doors when the relationship between variables is complex or nonlinear. \n",
    "\n",
    "Instead of modeling the functional form directly, **matching** pairs treated and untreated observations with similar values of a confounding variable, $Z$. \n",
    "\n",
    "This allows estimation of treatment effects, like ATT, even without knowing the true form of the relationship between treatment and outcome. \n",
    "\n",
    "The section also covers **weighted matching** as a way to account for distributional imbalances and introduces key concepts like distance matching and propensity score matching, which help formalize similarity between observations using one or more covariates. \n",
    "\n",
    "### Learning Objectives \n",
    "- Explain matching and its application to ATE, ATT, and ATUT \n",
    "- Perform inverse probability weighting and propensity score matching "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b52cd",
   "metadata": {},
   "source": [
    "### 1.1 Lesson: Another Way to Close Back Doors\n",
    "What if the relationship between treatment and outcome isn’t something we can easily model with a straight line? In this video, we’ll explore how **matching** can help us estimate treatment effects—even when the relationship between variables is complex, messy, or nonlinear. \n",
    "\n",
    "#### Matching\n",
    "With regression, we close back doors by finding a linear relationship between the outcome and the treatment, as well as with any confounders. But what if there isn’t a linear relationship, and we’re not sure how to model the situation? \n",
    "\n",
    "For example, what if:\n",
    "\n",
    "$$ Y = \\frac{2X + X^2}{Z} + \\varepsilon(X, Z) $$\n",
    "\n",
    "Then, we *could* model this linearly by including the term $\\frac{X^2}{Z}$  as an additional covariate, but what if we don’t think to do this? It’s a pretty weird relationship — we’re unlikely to think of it unless our domain knowledge hints of it:\n",
    "\n",
    "One solution would be matching: \n",
    "\n",
    "1. For each item with $X = 1$, we note its $Z$ value.\n",
    "2. Then we find another item with approximately the same $Z$ value, and match them. \n",
    "\n",
    "Suppose the $X = 0$ values have $Z = 1, 2$, while the $X = 1$ values have $Z = 1, 2, 3$. We throw out the $Z = 3$ item (it doesn't match), and we get: \n",
    "\n",
    "$Y(X = 0, Z = 1) \\; \\& = 0$\n",
    "\n",
    "$Y(X = 0, Z = 2) \\; \\& = 0$\n",
    "\n",
    "$Y(X = 1, Z = 1) \\; \\& = 3$\n",
    "\n",
    "$Y(X = 1, Z = 1) \\; \\& = 2.5$\n",
    "\n",
    "In this scenario the estimated effect is the difference between the average $X = 1$ and $X = 0$ values, or \n",
    "\n",
    "$$  \\frac{ (\\frac{3. + 2.5}{2} - \\frac{0}{2} )}{2} \\; = \\; 1.375$$\n",
    "\n",
    "This is the average treatment effect on the treated, since we are including counterfactuals (matches) for all treated items, but we are not including counterfactuals for all untreated items ($Z = 3$ has no counterfactual). Matching works even though we do not know the true relationship between $X$, $Y$, and $Z$. All we need to know is that $Z$ is the only confounder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960de87",
   "metadata": {},
   "source": [
    "### 1.2 Lesson: Weighted Averages\n",
    "We can also match by **weighting** \n",
    "\n",
    "In this case, if the untreated group has $Z = 1, 2, 3$ while the treated group has $Z = 1, 2$ \n",
    "\n",
    "Then we could weight the untreated values according to how close they are to the treated values: \n",
    "- For the untreated items, maybe the $Z = 1$ item would have a weight of 0.4, \n",
    "- $Z = 2$ has $0.4$, and \n",
    "- $Z = 3$ has 0.2 because $Z = 3$ is farther away from the treated $Z$ values. \n",
    "\n",
    "The exact weight is somewhat arbitrary: \n",
    "- Of the untreated items, $Z = 3$ should get a lower weight than $Z = 2$ because it’s farther from the treatment samples’ $Z$ values, but what weight, exactly, should it get?  \n",
    "\n",
    "There are many ways of calculating this, but in the end, it’s up to you. There is no one right answer. \n",
    "- For the treated items\" $Z = 1$ and $Z = 2$ both get an equal weight of $0.5$ \n",
    "- Then, to compute the effect, we weight each $Y$ value by the associated weight based on the $Z$ value. \n",
    "\n",
    "Another example could be if $Z$ is categorical: \n",
    "\n",
    "There are men and women. \n",
    "- If there are more men in the untreated group than in the treated group, we should downweight the untreated men (say, before computing the mean of ﻿Y﻿ or other statistic) according to the ratio of the count.\n",
    "- If there are also more women in the untreated group, we would downweight the untreated women, too, but likely by a different amount. \n",
    "- if the treated group has 8 men and 2 women, while the untreated group has 4 men and 6 women, then we could compute the ATT by weighting the untreated group so that men get a total weight of 0.8 and women 0.2. - This would involve weighting men at 0.2 each and women at 0.0333 each. The treated group would weight everyone at 0.1. In this way, we effectively have one counterfactual for each man and woman in the treated group. \n",
    "\n",
    "Taken together, these weights would allow us to compute the mean or other statistic of the untreated group as if it had the same distribution of men and women as the treated group. \n",
    "\n",
    "#### A Single Matching Variable\n",
    "As noted above, there’s a significant problem with trying to match the “nearest” $Z$ value between treated and untreated groups: \n",
    "- We need to know which $Z$ value is closest (and there may be multiple ways of measuring that). \n",
    "- When we are weighting, we need to know what weights to assign based on how close the $Z$ value is to other $Z$ values.\n",
    "\n",
    "The simplest approach is **distance matching:**\n",
    "- Distance Matching matches east treatment item to the closest untreated item(s) in distance, according ot the proximity of the matching variables (i.e., confounders like $Z$).\n",
    "- If there is one matching variable only, this is easy; but if there are several matching variables, we have to define a distance between samples where there are multiple covariates ($Z_1, Z_2, Z_3,$ etc.)\n",
    "- One way would be to standardize them, then take the euclidian distance, like $ \\sqrt{(Z_{1,T} - Z_{1, U})^2 + (Z_{2,T} - Z_{2, U})^2 + (Z_{3,T} - Z_{3, U})^2}$.\n",
    "    - Standardization is important because if the difference for $Z_1$ are on the order of 100, but for $Z_2$they are on the order of 1, then the $Z_1$ differences will completely outweight the $Z_2$ differences.\n",
    "    - This is especially true with Euclidean distances!, $\\sqrt{100^2 + 1^2}$  is $100.005$ ; the $1$ doesn't really matter because of the squaring.\n",
    "\n",
    "Perhaps the most common approach is **propensity score matching:**\n",
    "- In propensity score matching observations are similar if they were equally likely to be treated. \n",
    "- For example: \n",
    "    - if the matching variable is income, and the likelihood of treatment is normally distributed with mean $50,000, then the values $40,000 and $60,000 are “similar” because they are equally likely to be treated. - So, we can match a treated variable with income $40,000 to an untreated variable with $40,000 or with $60,000. This works to close the back door, although this may not be obvious. \n",
    "    \n",
    "To see why, suppose we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f88db34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Income</th>\n",
       "      <th>Target value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Treated</td>\n",
       "      <td>40000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Treated</td>\n",
       "      <td>50000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Treated</td>\n",
       "      <td>50000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Treated</td>\n",
       "      <td>60000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Untreated</td>\n",
       "      <td>40000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Group  Income  Target value\n",
       "0    Treated   40000            12\n",
       "1    Treated   50000            18\n",
       "2    Treated   50000            18\n",
       "3    Treated   60000             0\n",
       "4  Untreated   40000            12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dict = {\n",
    "    'Group' : ['Treated', 'Treated', 'Treated', 'Treated', 'Untreated', 'Untreated', 'Untreated', 'Untreated','Untreated', 'Untreated',],\n",
    "    'Income' : [40000, 50000, 50000, 60000, 40000, 40000, 50000, 50000, 60000, 60000],\n",
    "    'Target value' : [12, 18, 18, 0, 12, 12, 18, 18, 0, 0]\n",
    "    }\n",
    "df = pd.DataFrame(dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ab276",
   "metadata": {},
   "source": [
    "Then, taking the overall means results in the treated target value mean=12 and untreated=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee452e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Target value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Treated</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Untreated</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Income  Target value\n",
       "Group                           \n",
       "Treated    50000.0          12.0\n",
       "Untreated  50000.0          10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Group').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90843f42",
   "metadata": {},
   "source": [
    "But the income is a *confounder*; the two groups are actually the same when controlling for income. (This must be the case: the incomes’ targets match exactly across both groups.) \n",
    "\n",
    "If we match incomes using distance matching, for distance = 0, the untreated group gets 12, 18, 18, 0, and is the same target mean as the control group. \n",
    "\n",
    "This would involve the treated item with income $40,000 matching to the average of the two untreated items, with income $40,000 (all three of these have target = 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc9da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data means by treatment status:\n",
      "Group\n",
      "Treated      12.0\n",
      "Untreated    10.0\n",
      "Name: Target value, dtype: float64\n",
      "\n",
      "Matched data means:\n",
      "Treated mean: 12.0\n",
      "Untreated (matched) mean: 12.0\n",
      "Treatment effect: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/nl2zz_ln4d55dcscl_vvjglh0000gn/T/ipykernel_79623/373636855.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  matched_untreated = pd.concat([matched_untreated, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with numeric treatment indicator\n",
    "df_numeric = df.copy()\n",
    "df_numeric['Treatment'] = (df_numeric['Group'] == 'Treated').astype(int)\n",
    "\n",
    "# Separate the data into treated and untreated groups\n",
    "treated = df_numeric[df_numeric['Treatment'] == 1]\n",
    "untreated = df_numeric[df_numeric['Treatment'] == 0]\n",
    "\n",
    "# Create a new dataframe for matched pairs\n",
    "matched_treated = treated.copy()\n",
    "matched_untreated = pd.DataFrame(columns=df_numeric.columns)\n",
    "\n",
    "# For each treated observation, find matching untreated observations\n",
    "for _, treated_row in treated.iterrows():\n",
    "    # Find untreated observations with the same income (distance = 0)\n",
    "    matches = untreated[untreated['Income'] == treated_row['Income']]\n",
    "    \n",
    "    # If we found matches, add them to our matched dataset\n",
    "    if not matches.empty:\n",
    "        # Calculate the average of the matched untreated observations\n",
    "        avg_target = matches['Target value'].mean()\n",
    "        avg_income = matches['Income'].mean()\n",
    "        \n",
    "        # Create a new row for the matched untreated observation\n",
    "        new_row = pd.DataFrame({\n",
    "            'Group': ['Untreated (Matched)'],\n",
    "            'Income': [avg_income],\n",
    "            'Target value': [avg_target],\n",
    "            'Treatment': [0]\n",
    "        })\n",
    "        \n",
    "        # Add this match to our matched untreated data\n",
    "        matched_untreated = pd.concat([matched_untreated, new_row], ignore_index=True)\n",
    "\n",
    "# Combine the matched treated and untreated data\n",
    "matched_data = pd.concat([matched_treated, matched_untreated], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Original data means by treatment status:\")\n",
    "print(df.groupby('Group')['Target value'].mean())\n",
    "\n",
    "print(\"\\nMatched data means:\")\n",
    "print(\"Treated mean:\", matched_treated['Target value'].mean())\n",
    "print(\"Untreated (matched) mean:\", matched_untreated['Target value'].mean())\n",
    "print(\"Treatment effect:\", matched_treated['Target value'].mean() - matched_untreated['Target value'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e7e8b",
   "metadata": {},
   "source": [
    "But what about propensity score matching? \n",
    "\n",
    "This would mean that we can randomly cross-match untreated-$40,000 and untreated-$60,000 to treated-$40,000 and treated-$60,000 if we like because they have the same propensity to be treated (one-third of each are treated, compared with one-half for $50,000). \n",
    "\n",
    "This will produce the same target value average in the untreated matched group as for distance matching (in this case, 12). \n",
    "\n",
    "The propensity score approach involves treated-$40,000 matching to untreated-$40,000 or untreated-$60,000 and untreated-$60,000 to untreated-$40,000 or untreated-$60,000. In other words, the two untreated values are equally likely to be chosen. \n",
    "\n",
    "The distance approach involves treated-$40,000 matching to untreated-$40,000 and treated-$60,000 matching to untreated-$60,000. So again, the two untreated values are equally likely to be chosen (they are chosen once each). \n",
    "\n",
    "Then, having a 50% likelihood of picking $40,000 or $60,000 for the match gives the same average target value as always picking exactly one $40,000 and one $60,000. \n",
    "\n",
    "An alternative perspective is that the confounder (income) influences the propensity score (which is 33% for the high and low income, 50% for the medium one), which in turn influences the treatment. If we think of this in terms of directed acyclic graphs, we’ll see that the path through income toward the outcome (﻿Y﻿) must pass through the propensity score. Therefore, controlling for the propensity score suffices to control for this path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035588ae",
   "metadata": {},
   "source": [
    "## Topic 2: Practical Considerations and Advanced Techniques\n",
    "This section explains how to calculate propensity scores and use them effectively in matching and weighting strategies for causal inference. \n",
    "- You'll learn how logistic regression or binning can estimate treatment probabilities and explore practical decisions like how many matches to select, whether to match with or without replacement, and how to assign weights based on distance using kernels like Epanechnikov. \n",
    "- The section also introduces inverse probability weighting (IPW), considerations for caliper selection, matching in high-dimensional spaces, and advanced techniques like Mahalanobis distance, coarsened exact matching, and entropy balancing. \n",
    "- Finally, you'll see how to validate your propensity score models through stratification tests and how machine learning can enhance score estimation. \n",
    "\n",
    "### Learning Objectives \n",
    "- Perform inverse probability weighting and propensity score matching  \n",
    "- Explain matching variants such as the Mahalanobis distance and the Epanechnikov kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e38978",
   "metadata": {},
   "source": [
    "### 2.1 Lesson: Propensity Scores and Matching Strategies\n",
    "When people receive a treatment, it’s often not random — it’s influenced by their characteristics. In this video, you’ll learn how propensity scores help us account for those differences and how we can use them — along with a method called inverse probability weighting — to estimate causal effects more accurately.\n",
    "\n",
    "#### How to Calculate Propensity Scores\n",
    "With one covariate (such as income in the example above), we’d often just match on the value of the covariate itself. But if we wanted to use propensity score, we could compute it in one of two ways:\n",
    "- Perform logistic regression to model the probability of treatement. Above, we'd want to fit a quadratic function where:\n",
    "$$\\text{Prob} (40000) = \\frac{1}{3}, \\text{Prob} (50000) = \\frac{1}{2}, \\text{Prob} (60000) = \\frac{1}{3}$$\n",
    "- It has to be quadratic, including a $Z^2$ term, because we want it to be small for both high and low incomes and larger for the middle incomes.\n",
    "- Bin the data(in this case, perhaps into bins of 10,000) and compute the probability of treatment in each bin.\n",
    "\n",
    "#### Selecting Matches or Constructing a Matched Weighted Sample?  \n",
    "Selecting matches means selecting one or more untreated observations for each treated observation (or vice-versa). \n",
    "\n",
    "In a matched weighted sample, each untreated observation is assigned a numerical “weight” that determines how important it is. \n",
    "\n",
    "The “selecting matches” effect can vary more with small changes to the dataset as the matched observations can be “in” or “out” according to small differences in $Z$ values. \n",
    "\n",
    "Suppose a treated item has $Z = 0.55$, and the two nearest untreated items have $Z = 1$ and $Z = 0$. \n",
    "\n",
    "This treated item is closer to $Z = 1$ than to $Z = 0$. But a small change to $Z = 0.45$ can make it closer to $Z = 0$.\n",
    "\n",
    "#### If We’re Selecting Matches, How Many? \n",
    "\n",
    "Suppose we compute the ATT so that we find untreated matches (counterfactuals) for each treated item. There might be multiple good matches for each treated item. What do we do in that case? There are several options. \n",
    "\n",
    "1. Pick the best match. \n",
    "2. Pick the top k best matches (k-nearest-neighbor matching). We can then average their $Y$ values in order to find the effect. \n",
    "3. Pick all the matches within a given distance (radius matching). We can find all untreated $Z$ values within a radius of a given treated $Z$ value. Again, we average their $Y$ values. Radius matching can assign a varying number of matches to each treated item, so we’d want to weight these matches so that the effective number of matches to each treated item is 1. If one treated item has two matches and another has three, we’d weight the former by 0.5 each and the latter by 0.333 each. \n",
    "\n",
    "We can also match “with replacement,” meaning that a given untreated observation can match to multiple treated observations, or “without replacement,” meaning that each untreated observation can only match to one treated observation. \n",
    "\n",
    "One suggestion: If your untreated group contains many more observations than your treated group, it will make more sense to assign multiple untreated observations to each treated one. If your untreated group contains about the same number of observations as your treated group, you might not have enough observations to do that. \n",
    "\n",
    "#### If We’re Constructing a Matched Weighted Sample, How Will Weights Decay with Distance?\n",
    "In an example earlier, I said that the farther-away $Z = 3$ value would get a weight of $0.2$, while the closer ones got a weight of $0.4$ each. That was just a random guess — is there any more systematic way to assign weights based on distance?\n",
    "\n",
    "One approach is called the **Epanechnikov kernel**, which assigns a weight given the distance $x:$\n",
    "\n",
    "$K (x) \\; = \\; \\frac{3}{4} (1 - x^2)$\n",
    "\n",
    "In this case, since $Z = 3$ was a distance of 1 away from $Z = 2$, we'd have:\n",
    "\n",
    "$K(3 - 2) \\; = \\; \\frac{3}{4} (1 - (3-2)^2)$\n",
    "\n",
    "$ = \\; \\frac{3}{4} (1 - 1) \\; = \\; 0$ (So the $Z = 3$ value would be too far away to be included). \n",
    "\n",
    "If it were $Z = 2.5$ instead, we'd have $K(2.5 - 2) \\; = \\; \\frac{3}{4} (1 - (0.5)^2) \\; = \\; 0.5625$ as its weight. We might want to rescale the kernel if our typical distances are too large for too small for this kernel to be meaningful. \n",
    "\n",
    "The Epanechnikov kernel is similar to doing radius matching with a radius of 1, excep that the weight increases continuously up to this radius rather than assigning a disrete value of either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b249554",
   "metadata": {},
   "source": [
    "### 2.2 Lesson: Advanced Matching and Weighting Techniques, Part I\n",
    "\n",
    "#### Inverse Probability Weighting (IPW)\n",
    "This approach is used with propensity scores. \n",
    "- The weight for a given observation is equal to one divided by its probability. \n",
    "- For a treated observation, we use the probability that it would be treated. \n",
    "- For an untreated observation, we use the probability that it would be untreated. \n",
    "- This probability could be calculated using either (a) logistic regression based on one or more $Z$ values or (b) if $Z$ is categorical, we can simply count up the treatment probability for the given $Z$.\n",
    "\n",
    "**\"Treatment value”** means **“treated”** or **“untreated\":** \n",
    "- In other words, if items with income 40,000 have a propensity score of 33% (one-third of them are treated, and two-thirds are untreated) \n",
    "- Then they are weighted by $\\frac{1}{0.33} = 3$ in the treatment group, and $\\frac{1}{0.67} = 1.5$ in the control group.\n",
    "-  The result is that the total weight in the treatment and untreated group is, on average, the same (it is two times the number of items).\n",
    "- That's because for (on average) $Np$ items in *treatment* and $N(1 - p)$ items in the untreated grup, the total weight is $N \\cdot \\frac{p}{p} = N$ in treatement, and $N \\frac {1 - p}{1 - p} = N$ in the untreated group. \n",
    "\n",
    "In effect, if we have $N$ items of a certain type \\(around income \\$40,000, or having a similar propensity score to that\\), then inverse probability weighting assigns weights so that the total weight of these items is $2N$ , i.e., the average weight of each item is 2. \n",
    "\n",
    "What’s meant here is not that the items’ weights must total exactly $2N$ but that if:\n",
    "- (1) a number of samples were taken from the population and \n",
    "- (2) the model for computing the propensity is well-specified, with the correct formula, then on average, the total will be $2N$. \n",
    "\n",
    "\n",
    "\n",
    "This means that we are computing the average treatment effect (each item counts for 2, whether it is treated or untreated) instead of the average treatment effect on the treated (each treated item counts for a fixed amount, and untreated items count according to their treated match).\n",
    "\n",
    "Let’s consider the example above. If items with income 50,000 have a propensity score of 50%, they are weighted by $\\frac{1}{0.5} = 2$ in the treatment group and $\\frac{1}{1 - 0.5} = 2$ in the control group. In the above example, applying IPW to each sample, this results in a $Y$ value of \n",
    "\n",
    "$$\\frac {12 \\times 3 + 18 \\times 2 + 18 \\times 2 + 0 \\times 3}{10} = 10.8$$\n",
    "\n",
    "The same $Y$ value for treated and untreated.\n",
    "\n",
    "Let’s consider this special case where the treated and untreated items have the same target value given the same propensity score. There are $N_T$ of a given matching value in treated, which all have target value $T$ , and $N_U% in untreated, which also have target value ﻿T﻿ , then (assuming the statistical distribution of the data exactly reflects the \n",
    "\n",
    "\n",
    "#### What Is the Worst Acceptable Match?\n",
    "What if one treated observation is totally different from all untreated observations? \n",
    "- We define a “caliper” or “bandwidth,” which is a number of standard deviations (either of the propensity score or of the matched value, whichever you are using to match; in the latter case, I am assuming that the matched covariate is a single number ﻿Z﻿ so that it has a single standard deviation). \n",
    "- You can use the Epanechnikov kernel, which automatically has a caliper of 1. \n",
    "- You can use exact matching, which automatically has a caliper of 0. (This is unsuitable for continuous data values that rarely or never take on the same value.) \n",
    "- You can use coarsened exact matching, where you bin the data and then require that the two matches (treated and untreated) be in the same bin. (This is suitable for continuous data values.) \n",
    "\n",
    "Recall the **bias-variance tradeoff:** \n",
    "- High bias means that our prediction is systematically wrong — even if we took an infinite number of samples and averaged the results, it would still be wrong. \n",
    "- High variance means that our prediction varies dramatically when we take different samples. The wider the bandwidth, the more bias because the $Y$ values in the large bin may differ systematically from each other. \n",
    "- However, the wider the bandwidth, the less variance because we sample more possible matches. In other words: \n",
    "    - if we only take a single $Y$ value from a small bin, it should be correct on average but might be wrong by chance. \n",
    "    - If we take many matching $Y$ values from a large bin, the average will be stable, but that average might differ systematically from the correct value. \n",
    "    - Another way of putting it is that if the matches are at a distance of 1, 2, 3, 4, and 5 away (in terms of the matching value), then using just the distance 1 value has low bias (because we’re using the best estimator available) but high variance (we’d need to take an average of multiple estimates to reduce the variance.) \n",
    "\n",
    "The bias-variance tradeoff is generally important when performing matching! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bae5f0",
   "metadata": {},
   "source": [
    "### 2.3 Lesson: Advanced Matching and Weighting Techniques, Part II\n",
    "As noted above, distance matching becomes trickier with **multiple variables**. The solution mentioned was to use the Euclidean distance. \n",
    "\n",
    "Another option is to use the **Mahalonobis distance** between two observations, which is the square root of the following inner product: the displacement vector between the two d-dimensional matching variable vectors, multiplied by the inverse $S^{-1}$ of the covariance matrix for all the matching variables, multiplied again by the same displacement vector. \n",
    "\n",
    "The advantage of this approach (over the Euclidean distance) is that it handles correlated variables well: \n",
    "- Suppose $Z_1$ and $Z_2$ are highly correlated confounders; they are almost equal. \n",
    "- Then, when two samples differ the same in $Z_1$ as in $Z_2$, that difference is counted less. \n",
    "- When a sample is higher in $Z_1$ but lower in $Z_2$ (i.e., they differ in a way opposite to the usual correlation), then that difference is counted more. \n",
    "\n",
    "That way, differences that are contrary to the correlation (which are likely smaller) have a chance to matter. \n",
    "\n",
    "Another point is that the inclusion of the $S^{-1}$ term will tend to reduce the importance of features that are correlated to other features. \n",
    "\n",
    "The **curse of dimensionality** may make it harder to find matches: \n",
    "- In high-dimensional spaces, almost all points are approximately the same distance from all other points. This makes matching almost meaningless. \n",
    "- In effect, if there are many $Z$ values, then it’s unlikely that all of them will be particularly large or small. On average, they will be about average. \n",
    "- When we combine the various $Z$ values, the Euclidean distance will likely be around the average distance, so we will have little basis for matching. \n",
    "\n",
    "Here’s how to counter the curse of dimensionality: \n",
    "1. Leave some of the matching variables ($Z$) out, thereby reducing the number of dimensions. However, in that case, these particular confounders will not be controlled for. \n",
    "2. Include more observations, assuming you have some more to include. In an $N$-dimensional space, you need exponentially many samples ($2^N$) to make it likely that one sample is suitably close to another. (Imagine a cube divided up into eight cubes of half the width. The number eight comes from $2^3 = 8$.) \n",
    "3. Have a larger caliper/bandwidth, thus finding more matches. It is possible to choose the bandwidth needed to get the number of matches you want. However, this does not really solve the curse of dimensionality; the number of matches may be very sensitive to the chosen bandwidth. That is, you may get almost no matches for small bandwidths and almost everything for larger ones. \n",
    "\n",
    "Alternatively, we can do coarsened exact matching, where we put all variables into bins, and matches must have the same bins in all of the matching variables. This requires a really big sample size. But then, multiple matching variables always tends to require a large sample size unless you just increase the bandwidth and accept the really large number of matches. \n",
    "\n",
    "#### Entropy Balancing\n",
    "This means enforcing a restriction like “no difference in the mean of matching variable ﻿X﻿ between treatment and control.” We just need a set of weights that satisfies all restrictions. We can, in general, do any moment (recall that mean, variance, skew, and kurtosis are examples of moments). Thus, we could require no difference in variance, etc.\n",
    "\n",
    "#### Propensity Score Weighting with Multiple Matching Variables\n",
    "As mentioned above, you can estimate propensity scores by **regression**, such as logit or probit: \n",
    "- If the matching variable $Z$ can be thought of as influencing treatment via treatment propensity (which it really ought to be — that’s what treatment propensity is), then controlling for the propensity score should close all doors related to the matching variable. \n",
    "- To check this, within each bin or interval of the propensity score, you can check that the matching variable is unrelated to treatment. \n",
    "- For example, in the above case, since $40,000 and $60,000 are in the same propensity score bin (score = 1/3), we require that they have the same likelihood of treatment. \n",
    "    - If there is still a relationship, it might mean that the matching variable is not included in the regression correctly. (The propensity score is calculated incorrectly.) Perhaps you need more polynomial or interaction terms. This is called a **stratification test**. \n",
    "\n",
    "The point of the stratification test is that for a given interval of propensity score, the covariates used for matching ($Z$) should not be correlated with treatment. Otherwise, it might mean that we miscalculated the propensity score. \n",
    "\n",
    "You can also use machine learning to estimate the propensity score, including regularized regression and boosted regression. Boosted regression is a version of logistic regression that works similarly to gradient boost. The “boosting” means that it runs itself repeatedly, fixing its own remaining errors each time it runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255003bf",
   "metadata": {},
   "source": [
    "### 2.4 Lesson: Practical Considerations and Advanced Techniques\n",
    "\n",
    "#### Assumptions for Matching\n",
    "\n",
    "**Conditional Indepdence Assumption:**\n",
    "This is just the assumption that all back doors are closed. The set of matching variables you’ve chosen (covariates like $Z$) is enough to close all back doors. All remaining relationships between treatment and outcome is a desired (front door) relationship.\n",
    "\n",
    "**Common Support:**\n",
    "There should be enough control observations to match with. This is called **“common support.”** \n",
    "- You can imagine this could fail quite easily. \n",
    "- For example, imagine that in the treatment group, there are $Z$ values of $Z$ = 1, 5, and 10, while in the untreated group there are $Z$ values of 1, 11, and 12. We can easily find matches for $Z$ = 1 and $Z$ = 10 in the treated group but not for $Z$ = 5.\n",
    "\n",
    "**How to Check for Common Support**\n",
    "1. Look at the probability distribution of a variable for the treated and untreated groups. If one distribution is zero (or very small) where the other is nonzero, then we can’t use matching or weights to compare one to the other in that region. In the example above, this would be like $Z$ = 5, where the treatment distribution is nonzero while the untreated distribution is zero. However, to really talk about a probability distribution, it would be nice to have a denser array of samples. In the above data, we really cannot say whether $Z$ = 1, 5, and 10 means that we should be able to find $Z$ values ranging from 1 to 10 or whether those specific values are special.\n",
    "2. Another option is to look within a certain caliper or radius of each treated item and see if it finds support within that radius.\n",
    "\n",
    "Note that some approaches will automatically drop items lacking support, such as with calipers. However, propensity score matching with inverse probability weighting won't do this since an item with no support will simply have $P = 1$ (it was certain to be treated) and will be given a weight $\\frac{1}{P} = \\frac{1}{1} = 1$. That's smaller than other weights, but it is still not zero.\n",
    "\n",
    "If we don’t have any matches, then we aren’t really doing matching — even though we are technically following the instructions. When this happens, you may want to drop the items without matches. In fact, you might want to require more than one match — perhaps there should be a certain number of matches within each propensity score bin for the untreated variable; otherwise, you skip that bin. \n",
    "\n",
    "Commonly, the bins you skip will have very high or very low propensity scores because these will be the scores that are least likely to have a match. \n",
    "\n",
    "In any case, the only way to fix common support issues is to drop the treated samples that find no match and/or the untreated samples that are present in low enough numbers that the match isn’t good enough. If you have to drop too much data, then you aren’t really computing an ATE or ATT anymore. At some point, you have diverged enough from your goal that matching may not work for you.\n",
    "\n",
    "#### Balance\n",
    "Balance is the assumption that we have matched correctly — we have common support, and we are using covariates $Z$ that close all backdoors. Whereas conditional independence means that we could, in principle, match correctly, balance means that we have matched correctly. If there is not common support, we may not be able to match correctly in practice. \n",
    "\n",
    "With interaction terms, it is more likely to fail to find common support and thus to fail to get balance. For instance, if we have ($X$, $Z$) = (1, 0) and (0, 1) in treatment but (0, 0) and (1, 1) in the untreated group, then $X$ and $Z$ have support individually (the $X$ = 1 matches to the $X$ = 1) but not jointly (the (1, 0) does not match to any (1,0)). \n",
    "\n",
    "A balance table displays moments such as the mean and standard deviation. If the mean is the same for both treatment and control group, then it is likely that balance is good. If the means are quite different, they could show a lack of common support. After matching, the balance table should definitely show similar means and similar standard deviations.\n",
    "\n",
    "#### Estimating Mean Differences\n",
    "We have discussed how to estimate the effect: simply find the $Y$ values of the matched treated and untreated data, perhaps weighted appropriately. But what about standard errors? \n",
    "\n",
    "The two problems here are: \n",
    "- The errors of the weights themselves contribute to the overall error. \n",
    "- If we dropped some observations, we can’t tell how the choice of observations to drop may have influenced the error. \n",
    "\n",
    "The best way to compute the standard error in this case is to use a bootstrap simulation. However, if you do this, you’d have to automate the match selection process and incorporate it into the bootstrap — the whole matching process would have to be repeated.\n",
    "\n",
    "#### Combining Matching and Regression \n",
    "We can combine matching with regression using the following techniques: \n",
    "- Regression adjustment \n",
    "- Double robust estimation, which works even if regression or matching alone doesn’t close all the back doors. \n",
    "    - E.g., AIPWE (augmented inverse probability weighted estimator) \n",
    "    - E.g., entropy balancing\n",
    "\n",
    "#### Matching and Treatment Effects\n",
    "Regression typically gives about the same treatment effect whether you do ATE, ATT, or ATUT. That’s because a particular beta value is the effect. Matching, in contrast, can give you different treatment effects. To find ATE, use inverse probability weighting. \n",
    "\n",
    "To find ATT, weight all treatment observations the same or (if you are selecting matches) find a set of untreated matches for each treatment observation. \n",
    "\n",
    "For inverse probability weighting with ATT, the weighting involves giving the treated group a weight of 1, and the untreated group gets $\\frac{p}{1 - p}$ instead of $\\frac {1}{1 - p}$. \n",
    "\n",
    "To find ATUT, find a set of treated matches for each untreated observation. The weighting involves giving the untreated group a weight of 1 and the treated group gets $\\frac{1 - p}{p}$ instead of $\\frac {1}{p}$.  \n",
    "\n",
    "You can also get the average treatment effect by averaging $p \\times \\text{ATT} + (1 - p) \\times \\text{ATUT}$, where $p$ is the proportion treated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0d828",
   "metadata": {},
   "source": [
    "### Key Terms and Definitions\n",
    "1. **Matching:** A method for estimating causal effects by pairing treated and untreated units with similar values of confounding variables to simulate a randomized experiment.\n",
    "2. **Average Treatment Effect on the Treated (ATT):** The average causal effect of a treatment on those who actually received the treatment. \n",
    "3. **Weighted Matching:** A method where untreated observations are weighted to match the distribution of confounders in the treated group rather than selecting exact matches.\n",
    "4. **Distance Matching:** A technique that matches treated and untreated observations based on the smallest distance (e.g., Euclidean) between their covariate values. \n",
    "5. **Propensity Score Matching:** Matching based on the probability of receiving treatment given covariates; units with similar scores are matched to control for confounding.\n",
    "6. **Propensity Score:** The probability of receiving the treatment, estimated using observed covariates (e.g., via logistic regression). \n",
    "7. **Epanechnikov Kernel:** A weighting function used in kernel-based matching that assigns weights based on the distance between units, favoring closer matches. \n",
    "8. **Inverse Probability Weighting (IPW):** A method where each observation is weighted by the inverse of the probability of receiving the treatment (or control) it actually received, to estimate ATE.\n",
    "9. **Caliper / Bandwidth:** A threshold distance used to restrict acceptable matches; matches outside this range are discarded to improve quality. \n",
    "10. **Coarsened Exact Matching:** A matching approach where covariates are binned into categories, and matches are made within the same bins across groups. \n",
    "11. **Bias-Variance Tradeoff:** A fundamental concept where tighter matching (low bias) may increase variability, while looser matching (low variance) may introduce bias. \n",
    "12. **Mahalanobis Distance:** A distance metric that accounts for correlations among covariates when matching, improving match quality in multivariate settings. \n",
    "13. **Curse of Dimensionality:** A challenge in high-dimensional matching where most points are approximately equidistant, making meaningful matching difficult. \n",
    "14. **Entropy Balancing:** A reweighting method that ensures exact balance of specified covariate moments (e.g., means) between treated and control groups. \n",
    "15. **Common Support:** The condition that there is sufficient overlap in covariate distributions between treated and untreated groups to allow meaningful matching. \n",
    "16. **Balance:** The degree to which covariate distributions are similar between treated and control groups after matching. \n",
    "17. **Balance Table:** A table comparing means (and often standard deviations) of covariates across groups to assess the success of matching. \n",
    "18. **Bootstrap Simulation:** A resampling technique used to estimate standard errors of treatment effects in matching procedures. \n",
    "19. **Regression Adjustment:** Using regression after matching to control for residual covariate differences and improve precision. \n",
    "20. **Double Robust Estimation:** A method that combines matching and regression; valid if either model is correctly specified. \n",
    "21. **Augmented Inverse Probability Weighted Estimator (AIPWE):** A specific double robust estimator that combines outcome modeling and propensity weighting. \n",
    "22. **Treatment Effects (ATE, ATT, ATUT):** \n",
    "-  ATE: average treatment effect across the entire population \n",
    "-  ATT: Effect on the treated group \n",
    "-  ATUT: Effect on the untreated group\n",
    "23. **Stratification Test:** A check to ensure that within strata (bins) of the propensity score, covariates are uncorrelated with treatment assignment. \n",
    "24. **Selecting Matches:** The process of choosing one or more untreated units for each treated unit (or vice versa), often using nearest neighbor, radius, or caliper methods. \n",
    "25. **Matching With/Without Replacement:** Whether an untreated unit can be matched to multiple treated units (with replacement) or only one (without replacement). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58879c98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
