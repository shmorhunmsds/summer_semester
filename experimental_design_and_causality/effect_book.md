## Chapter 1 - Designing Research

### 1.1 I have a question
A **research question** is a question that you have that you plan to answer, or at least try to answer, by doing research. Simple as that. Or, rather, as difficult as that. A good research question is well-defined, answerable, and understandable

- let’s say that our research question is “does adding an additional highway lane reduce traffic?”

### 1.2 Empirical Research
This book will focus on empirical research and, specifically, **quantitative empirical research**. Empirical research is any research that uses structured observations from the real world to attempt to answer questions. 
- So instead of trying to *reason* our way through what drivers would do if given an additional highway lane, we try to *observe* the choices that drivers take. Perhaps we interview drivers about how they make decisions. Or maybe we get a big dataset of traffic violations, or of traffic flow numbers on highways.

One particularly sticky problem with quantitative empirical research is that the numbers that we observe often don’t tell us exactly what we want to know.

*We’re probably interested in whether we can make traffic go down by turning a two-lane highway into a three-lane highway! But as much as we want them to, the numbers we have don’t actually tell us that right away. All we have are two-lane highways and three-lane highways. We don’t have a “what if” highway that tells us how much traffic there would have been if we’d made that two-lane highway one lane wider.*

This problem constitutes a major headache for us researchers. If the numbers we have don’t actually answer the research question we have, what can we do?

Well, it turns out that, if you do it right, you often can figure out how to collect the right numbers, or do the right things to those numbers, to get an actual answer to our question. But it doesn’t come free. We have to carefully design the right kind of analysis that will answer our question.

### 1.3 Why Research Needs a Design
Why is it so important for research to be properly designed? One way we can think about this is by looking at what happens when it’s not.

Let’s take our highways and traffic example. How might we go about researching an answer to this question?

*Our first pass might be to just compare traffic patterns on highways with more lanes against traffic patterns on highways with fewer lanes. Seems reasonable. But then you do it, and it turns out that more lanes seem to go along with more traffic!* 

*Sure, maybe additional lanes do lead to more traffic. But it takes research design to know that our first-pass analysis wasn’t right and to figure out what to do instead.*

A lack of solid research design can be seen in the results, as well.

## Chapter 2 - Research Questions

### 2.1 What Is a Research Question?
Coming up with a question is easy. Coming up with a good research question is much harder.

**A research question is a question that can be answered, and for which having that answer will improve your understanding of how the world works.**

What does it mean to have a question that can be answered? It means that it’s possible for there to be some set of evidence in the world that, if you found that evidence, your question would have a believable answer.

So we have a question that can be answered. But does it improve our understanding of how the world works? 

What this means is that the research question, once answered, should tell you about something broader than itself. It should inform theory in some way.

Theory just means that there’s a why or a because lurking around somewhere.

Take germ theory, for example. Germ theory says that microorganisms like bacteria and viruses can cause disease. This explains why we have diseases, and also why disease can spread from one person to another. We don’t call it “a theory” because we're uncertain about whether its true. We call it a theory because it tells us *why*.

A good research question takes us from theory to hypothesis, where a hypothesis is a specific statement about what we will observe in the world. That is, a research question shoudl be something that, if you answer it, helps improve your *why* explanation. Great research questions often come from the theory themselves. 

Let’s check our two conditions for research questions.
1. Could we answer this question?
2. Second, does this research question tell us about how the world works?

A good test for whether a research question informs theory is to imagine that you find an unexpected result, and then wonder whether it would make you change your understanding of the world.

This ability to see a bad result and still hold on to the original theory tells us that the research question wasn’t very good, at least not for this theory.66 This means if the Sesame Street study had turned out in favor of our theory we shouldn’t have increased our confidence either. 

A really good research question, once answered, should be hard to explain away just because it’s inconvenient.

### 2.2 Why Start with a Question?
why not skip the hard part of deriving a research question from a theory and instead just see what sorts of patterns are in the data? This is called *data mining*, and it turns out to be very good at somethings and very bad at others.
- Data mining is good at *finding patterns* and in *making predictions under stability*
- Data mining is less good at *improving our understanding*, (or helping to improve theory)
- Data mining also has a tendency to find *false positives* if you aren't careful.

Finding patterns and making predictions are very valuable. After all, there’s no way we can really theorize about every possible pattern that could be in the data and think to check it. Doing something that just asks what we see rather than why is the right angle to take there. Plus, sometimes seeing patterns in data can give us ideas for research questions that we can examine further in other data sources.

#### Why does Data Mining Have Difficulty Helping Theory?
- Data mining focuses on what's in the data, not *why* its in the data. 
- Its fantastic at revealing correlations - patterns in the data of how variables we've observed have varied together in the past - but the correlations it uncovers may have little to do with causality, or an understanding of why those variables move together. 
- Data mining is well-equipped to find the relationship but poorly-equipped to tell us why that relationship is there.
- Another reason is that, because it’s so focused on the data, data mining doesn’t really deal in abstraction.

#### False Positives
False positives are another reason why data mining can be dangerous. There are ways of avoiding false positives while doing data mining - this is something they worry about a lot in data science and have a lot of tools for. Data mining isn’t bad. It’s just bad as a final step if you’re trying to explain the world.

### Where do Research Questions Come From?
Research questions can come from lots of places. There are two steps in this process: 
1. thinking about theory
2. coming up with a research question. 

Either one can come first.

Perhaps it begins with theory. With the theory in place, the process continues with our hypothesis:

> *“if this is how the world works, what would I expect to see in the world?”*

Our theories might lead to research questions. These research questions tel us a hypothesis to test such that the result of that test tells us something about the theory. 


Let’s be honest, sometimes research questions also come from opportunity. Have a neat data set? Think about what data is available to you and whether any related research questions or theories come to mind.

### How do you know if you've got a good research question?
But is it really a good one? Just a few things to check before you get too far into the process:
1. Consider Potential Results. 
- A good way to double-check the relationship between your research question and your theory is to consider the potential answers you might get. 
- Then, imagine what kind of sense you’d make of that result, or what conclusion you would draw. Let’s say you find that students do tend to work harder in school when they’re paid for good grades. What would this tell us about how students respond to incentives? Or let’s say you find that students don’t work harder when they’re paid. What would that tell us about how students respond to incentives? 
- If you can’t say something interesting about your potential results, that probably means your research question and your theory aren’t as closely linked as you think! Let’s say we do find that kids who happen to play video games are more aggressive. Can we take that result and claim that video games are a cause of aggression? Not really, for the reasons we’ve discussed previously. So maybe that research question really isn’t linked to that theory very well.

2. Consider Feasibility
- A research question should be a question that can be answered using the right data, if the right data is available. But is the right data available? 
- If answering your research question is possible but requires following millions of people repeatedly for decades, or trying to measure something that’s really hard to measure accurately, like trying to get people to remember what they had for lunch three years ago, or getting access to the private finances of thousands of unwilling people, then that research question might not be feasible. 
- While sometimes you can get around these problems with a clever design, you might want to consider going back to the drawing board.

3. Consider Design
- A research question can be great on its own, but it can only be so interesting without an answer. So, an important part of evaluating whether you have a workable research question is figuring out if there’s a reasonable research design you can use to answer it. Figuring out whether you do have a reasonable research design is the topic of the rest of this book.

4. Keep it Simple
- Answering any research question can be difficult. Don’t make it even harder on yourself by biting off more than you can chew! 
- A common mistake is to bundle a bunch of research questions into one. “What are the determinants of social mobility?” I.e., how someone can move from one social class to another throughout their lifetime. There are many determinants of social mobility. You’re unlikely to answer that question well. Instead try “Is birth location a determinant of social mobility?” For another example, how about the question “How was the medium of painting affected by the Italian renaissance?” In a million ways! You’ll get lost and do a poor job on a bunch of minor pieces instead of getting at the whole. Instead maybe “What similar characteristics are there among the countries that adopted the use of perspective in painting most quickly?”

So, consider feasibility, scale, and design. Keep it simple, and think about whether the results you might likely see would tell you anything interesting about the world. After all, learning something interesting and new about the world is our goal!

___

### Chapter 3 - Describing Variables
This chapter will be all about how to describe a variable. It turns out that empirical research questions really come down entirely to **describing the density distributions of statistical variables.**

- A **variable**, in the context of empirical research, is a bunch of observations of the same measurement.
- Successfully describing a variable means being able to take those observations and clearly explain what was observed

#### 3.2: Types of Variables
The first step in figuring out how to describe a variable is figuring out what kind of variable it is. In general, the most common kinds of variables you will encounter are:
- **Continuous Variables:** Continuous variables are variables that could take any value (perhaps within some range).
- **Count Variables:** Count variables are those that, well, count something. Perhaps how many times something happened or how many of something there are.
- **Ordinal Variables:** Ordinal variables are variables where some values are “more” and others are “less”.
- **Categorical Variables:** Categorical variables are variables recording which category an observation is in. 
- **Binary Variables:** Categorical variables that only take two values. Often, these values are “yes” and “no.”

#### 3.3: The Distribution
Once we have an idea of what kind of variable we’re dealing with, the next step is to look at the **distribution** of that variable. A variable’s **distribution** is a description of how often different values occur.
- When it comes to **categorical** or **ordinal** variables, the variable’s distribution can be described by simply giving the percentage of observations that are in each category or value. The full distribution can be shown in a frequency table or bar graph, which just shows the percentage of the sample or population that has each value.
- **Continuous** variables are a little trickier. We can’t just do a frequency table for continuous variables since it’s unlikely that more than one observation takes any specific value. For **continuous** variables, distributions are described not by the probability that the variable takes a given value, but by *the probability that the variable takes a value close to that one.* One common way of expressing the distribution of a continuous variable is with a histogram.
- With a continuous variable we can go one step further than a histogram all the way to a density. A density shows what would happen to a histogram if the bins got narrower and narrower. When we have a density plot, we can describe the probability of being in a given range of the variable by seeing how large the area underneath the distribution is.
    - Once you have the distribution of the variable, that’s really all you can say about it until you start to incorporate how it relates to other variables. 

#### 3.4 Summarizing the Distribution
Once we have the variable’s distribution, we can turn our attention to summarizing that variable. The whole distribution might be a bit too much information for us to make any use of, especially for continuous variables. So our goal is to pick ways to take the *entire* distribution and produce a few numbers that describe that distribution pretty well.
- Probably the most well-known example of a single number that tries to describe an entire distribution is the mean.
    - So what the mean is actually doing is looking at the distribution of the variable and summarizing it, boiling it down to a single number. 
    - What is that number? The mean is supposed to represent a central tendency of the data - it’s in the middle of what you might get. 
    - More specifically, it tries to produce a representative value.
- If the goal is to describe the distribution to someone, why bother doing a calculation of the mean when we could just tell people about the distribution itself?
    - The *X*th percentile of a variable’s distribution is the value for which X% of the observations are less. We can see percentiles on our distribution graphs. We can actually describe the entire distribution perfectly this way. Pretty soon we'll have mapped the entire distribution by just shading in a little more each time. So percentiles are a fiarly direct way of describing a distribution.
- There are a few percentiles that deserve special mention.
    - The first is the **median**, or the 50th percentile. Like the mean, the **median** is measuring a central tendency of the data. Instead of trying to produce a representative value, like the mean does, the **median** gives a representative observation. The median is generally used over the mean when you want to describe what a typical observation looks like, or when you have a variable that is highly skewed, with a few really big observations.
    - The other two percentiles to focus on are the **minimum**, or the 0th percentile, and the **maximum**, or the 100th percentile. These are the lowest and highest values the variable takes.
- We need ways to describe variation in addition to central tendencies or percentiles. 
    - The way that variation shows up in a distribution graph is in how wide the distribution is. 
    - **Low variation:** If the distribution is tall and skinny, then all of the observations are scrunched in very close to the mean.
    - **High variation:** If it’s flat and wide, then there are a lot of observations in those fat “tails” on the left and right that are far away from the mean.

There are quite a few ways to describe variation. Some of them, like the mean, focus on values, and others, like the median and percentiles, focus on observations.
- **Variance** is a measure of variation that focuses on values and is derived from the mean. To calculate the variance in a sample of observations of our data, we:
1. Find the mean
2. Subtract the mean from each value
3. Square each of these values
4. Add them up
5. Divide by the number of observations minus 1
- The bigger the variance is, the more variation there is in that variable. iIn steps 4 and 5 previously that we’re sort of taking a mean. But the thing we’re taking the mean of is squared variation around the actual mean. So any observations that are far from the mean get squared - making them even bigger and count for more in our mean! In this way we get a sense of how far from the mean our data is, on average.
- One downside of the variance is that it’s a little hard to interpret, since it is in “squared units.” So we often convert the variance into the **standard deviation** by taking the square root of it to get us back to our original units.
- Figuring out how much variation one standard deviation is can be kind of tricky, and largely just takes practice and intuition. But a graph can help.

We can also compare percentiles to see how much a variable varies. All we have to do is pick a percentile above the median, and a percentile below the median, and see how different they are. The range can be very sensitive to really big observations, so its not a great measure. Instead, the most common percentile-based measure of variation you’ll tend to see is the **interquartile range**, or **IQR**. This gives the difference between the 75th percentile and the 25th percentile.

The IQR is handy for a few reasons: 
- First, you know that the value given by the IQR covers exactly half of your sample. So for the half of your sample closest to the median, the IQR gives you a good sense of how close to the median they are. 
- Second, unlike the variance, the IQR isn’t very strongly affected by big tail observations. So, as always, it’s a good way of representing observations rather than values.

**Skew** describes how the distribution leans to one side or the other.
- Distributions with a heavy right tail but no big left tail have a "right skew"
- Similarly, a distribution with lots of observations in the left tail would have a left skew
- A distribution with similar tails on both sides is symmetric.

Skew can be an important feature of a distribution to describe. It can also give us problems if we’re working with means and variances, since those really-huge values will affect any measure that tries to represent values. One way of handling skew in data is by **transforming** the data. If we apply some function to the data that shrinks the impact of those really-big observations, the mean and variance work better. A common transformation in this case is the **log transformation**, where we take the natural logarithm of the data and use that instead. This can make the data much better-behaved.

One reason the natural log transformation is so popular is that the transformed variable has an easy interpretation:
- A log increase of $0.1$ translates to approximately a $0.1 \times 100 = 1\% $ increase in the original variable.
- So an increase in log income from 10.00 to 10.02 in log terms means a $(10.02 - 10.00) \; = \; .02 \approx 2\%$ increase in income itself. 
- This approximation works pretty well for small increases like .01 or .02, but it starts to break down for bigger increases like, say, .2. Anything above .1/10% or so and you should avoid the approximation.

There might be trouble brewing if you take the log and it still looks skewed. This can be the case when you have fat tails, i.e., observations far away from the mean are very common. When you have fat tails on one side but not the other, this can make your data very difficult to work with indeed.

#### Theoretical Distributions
Statistics makes a very clear distinction between the truth and the data we’ve collected. But isn’t the data truly what we’ve collected? Well, sure, but what it is supposed to represent is some broader truth beyond that. We’ll need to pair our observed distributions, the ones we’ve been talking about so far in this chapter, with theoretical distributions of how data behaves under different versions of the truth.

**Quick Notation**
- English/Latin letters represent data. So $x$ might be a variable of actual observed data. That’s our 1000 surveys with parents about their kids’ sharing ages.
- Modifications of English/Latin letters represent calculations done with real data. A common way to indicate "mean" is to use a bar on top of the letter so $\bar{x}$ is the mean of $x$ we collected in our data. 
- Greek letters represent the *truth*. We don’t know what actual values these take, but we can make assumptions. ertain Greek letters are commonly used for certain kinds of truth:
    - $\mu$ commonly indicates some sort of mean.
    - $\sigma$ the standard deviation
    - $\rho$ for correlation
    - $\beta$ for regression on coefficients
    - $\epsilon$ for "error terms"
- Modifications of Greek letters represent our *estimate of the truth*. We don’t know what the truth is, but we can make our best guess of it.
    - The most common way to represent a guess is to put a "hat" on it:
        - So $\hat{\mu}$ is my estimate of what I think $\mu$ is. 
        - If the way that I plan to estimate $\mu$ is by taking the mean of $x$, then I would say $\hat{\mu} \; = \; \bar{x}$ 


The **theoretical distribution** is what generated your data. That’s actually a good way to think about theoretical distributions. They’re the distribution of all the data, even the data you didn’t actually collect, and maybe could never actually collect! 

If we want to learn about the average age children share at, the place that data comes from is the theoretical distribution. Remember, we don’t really care about the mean in our observed data, $\bar{x}$, we care about the true average $\mu$. 
- The reason we bother gathering data in the first place is because it will let us make an estimate $\hat{\mu}$ about what the theoretical distribution it came from is like.
- The second thing this “infinite observations” fact tells us is that the more observations we have, the better a job our observed data will do at matching that theoretical distribution.This means as we get more and more observations, we’re going to do a better and better job of getting an observed distribution that matches the theoretical one that we sampled the data from.

There are infinite different theoretical distributions, but some pop up in applied work often. There are some well-known distributions that are applied over and over again. If we think that our data follows one of these distributions we’re in luck, because it means we can use that theoretical distribution to do a lot of work for us!

The first to cover is the normal distribution. The normal distribution is symmetric (i.e., the left and right tails are the same size, there’s no skew/lean). The normal distribution often shows up when describing things that face real, physical restrictions, like height or intelligence.
- The normal distribution also pops up a lot when looking at aggregated values.
    - Income might have a strong right skew, but if we take the mean of income in one sample of data, aggregating across observations, and then again in another sample of data, aggregating across different observations, and then again and again in different samples, the *distribution of the mean across different samples* would have a normal distribution.
    - The normal distribution technically has infinite range, meaning that every value is possible, even if unlikely.

The second is a bit of a cheat, and it’s the log-normal distribution. The log-normal distribution has a heavy right skew, but once we take the logarithm of it, it turns out to be a normal distribution! How handy.
- The log-normal is a very convenient version of a skewed distribution, since we can take all the skew out of it by just applying a logarithm.
- Heavily skewed data comes up all the time in the real world. Anything that’s unequally distributed and that doesn’t have a maximum possible value is generally skewed (income, wealth…) When we see a skewed distribution, we tend to hope it’s log-normal for convenience reasons. So it’s a good idea, after taking the log of a skewed variable, to look at its distribution to confirm that it does indeed look normal.

How can we use our empirical data to learn about the theoretical distribution? 

Remember, our real reason for looking at and describing our variables is because we want to get a better idea of the theoretical distribution. We’re not really interested in the values of the variable in our sample, we’re interested in using our sample to find out about how the variable behaves in general.

We can, if we like, take our sample and look at its distribution (as well as its mean, standard deviation, and so on), figure that’s the best guess we have as to what the theoretical distribution looks like, and go from there.

One thing that is a bit easier to do is to learn whether certain theoretical distributions are unlikely. Maybe we can’t figure out exactly what the theoretical distribution is, but we can rule some stuff out. How can we figure out how likely a certain theoretical distribution is? We follow these steps:
1. Choose some description of the theoretical distribution - its mean, its median, its standard deviation, etc. Let’s use the mean as an example.
2. Use the properties of the theoretical distribution and your sample size to find the theoretical distribution of that description in random samples - means generally follow a normal distribution, and the standard deviation of that normal distribution is smaller for bigger sample sizes.
3. Make that same description of your observed data - so now we have the distribution of our theoretical mean, and we have the actual observed mean.
4. Use the theoretical distribution of that description to find out how unlikely it would be to get the data you got - if the theoretical distribution of the mean we’re looking at has mean 1 and standard deviation 2, and our observed mean is 1.5, we’re asking “how likely is it that we’d get a 1.5 or more from a normal distribution with mean 1 and standard deviation 2?”
5. If it’s really unlikely, then you probably started with the wrong theoretical distribution, and can rule it out. If we’re doing statistical significance testing, we might say that our observed mean is “statistically significantly different from” the mean of the theoretical distribution we started with.

We could also frame all of this in terms of hypothesis testing. Following the same steps, we can say:
- Step 1: Our null hypothesis is that the mean is 90. Our alternative hypothesis is that the mean is not 90.
- Step 2: Pick a test statistic with a known distribution. Means are distributed normally, so we might use a Z-statistic, which is for describing points on normal distributions.
- Step 3: Get that same test statistic in the data.
- Step 4: Using the known distribution of the test statistic, calculate how likely it is to get your data’s test statistic, or something even more extreme ($p$-value).
- Step 5: Determine whether we can reject the null hypothesis. This comes down to your threshold ($\alpha$), how unlikely does your data’s test statistic need to be for you to reject the null? A common number is 5%. If that's your threshold, then if step 4 gave you a lower $p$-value than your $\alpha$ threshold, then that's too unlikely for your, and you can reject the null hypothesis that the mean is 90. 

This section describes what hypothesis testing actually is and what it’s for. Statistical significance is not a marker of being correct or important. It’s just a marker of being able to reject some theoretical distribution you’ve chosen. That can certainly be interesting. At the very least, we’ve narrowed down the likely list of ways that our data could have been generated. And that’s the whole point!