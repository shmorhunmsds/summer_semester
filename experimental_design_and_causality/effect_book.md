## Chapter 1 - Designing Research

### 1.1 I have a question
A **research question** is a question that you have that you plan to answer, or at least try to answer, by doing research. Simple as that. Or, rather, as difficult as that. A good research question is well-defined, answerable, and understandable

- let’s say that our research question is “does adding an additional highway lane reduce traffic?”

### 1.2 Empirical Research
This book will focus on empirical research and, specifically, **quantitative empirical research**. Empirical research is any research that uses structured observations from the real world to attempt to answer questions. 
- So instead of trying to *reason* our way through what drivers would do if given an additional highway lane, we try to *observe* the choices that drivers take. Perhaps we interview drivers about how they make decisions. Or maybe we get a big dataset of traffic violations, or of traffic flow numbers on highways.

One particularly sticky problem with quantitative empirical research is that the numbers that we observe often don’t tell us exactly what we want to know.

*We’re probably interested in whether we can make traffic go down by turning a two-lane highway into a three-lane highway! But as much as we want them to, the numbers we have don’t actually tell us that right away. All we have are two-lane highways and three-lane highways. We don’t have a “what if” highway that tells us how much traffic there would have been if we’d made that two-lane highway one lane wider.*

This problem constitutes a major headache for us researchers. If the numbers we have don’t actually answer the research question we have, what can we do?

Well, it turns out that, if you do it right, you often can figure out how to collect the right numbers, or do the right things to those numbers, to get an actual answer to our question. But it doesn’t come free. We have to carefully design the right kind of analysis that will answer our question.

### 1.3 Why Research Needs a Design
Why is it so important for research to be properly designed? One way we can think about this is by looking at what happens when it’s not.

Let’s take our highways and traffic example. How might we go about researching an answer to this question?

*Our first pass might be to just compare traffic patterns on highways with more lanes against traffic patterns on highways with fewer lanes. Seems reasonable. But then you do it, and it turns out that more lanes seem to go along with more traffic!* 

*Sure, maybe additional lanes do lead to more traffic. But it takes research design to know that our first-pass analysis wasn’t right and to figure out what to do instead.*

A lack of solid research design can be seen in the results, as well.

## Chapter 2 - Research Questions

### 2.1 What Is a Research Question?
Coming up with a question is easy. Coming up with a good research question is much harder.

**A research question is a question that can be answered, and for which having that answer will improve your understanding of how the world works.**

What does it mean to have a question that can be answered? It means that it’s possible for there to be some set of evidence in the world that, if you found that evidence, your question would have a believable answer.

So we have a question that can be answered. But does it improve our understanding of how the world works? 

What this means is that the research question, once answered, should tell you about something broader than itself. It should inform theory in some way.

Theory just means that there’s a why or a because lurking around somewhere.

Take germ theory, for example. Germ theory says that microorganisms like bacteria and viruses can cause disease. This explains why we have diseases, and also why disease can spread from one person to another. We don’t call it “a theory” because we're uncertain about whether its true. We call it a theory because it tells us *why*.

A good research question takes us from theory to hypothesis, where a hypothesis is a specific statement about what we will observe in the world. That is, a research question shoudl be something that, if you answer it, helps improve your *why* explanation. Great research questions often come from the theory themselves. 

Let’s check our two conditions for research questions.
1. Could we answer this question?
2. Second, does this research question tell us about how the world works?

A good test for whether a research question informs theory is to imagine that you find an unexpected result, and then wonder whether it would make you change your understanding of the world.

This ability to see a bad result and still hold on to the original theory tells us that the research question wasn’t very good, at least not for this theory.66 This means if the Sesame Street study had turned out in favor of our theory we shouldn’t have increased our confidence either. 

A really good research question, once answered, should be hard to explain away just because it’s inconvenient.

### 2.2 Why Start with a Question?
why not skip the hard part of deriving a research question from a theory and instead just see what sorts of patterns are in the data? This is called *data mining*, and it turns out to be very good at somethings and very bad at others.
- Data mining is good at *finding patterns* and in *making predictions under stability*
- Data mining is less good at *improving our understanding*, (or helping to improve theory)
- Data mining also has a tendency to find *false positives* if you aren't careful.

Finding patterns and making predictions are very valuable. After all, there’s no way we can really theorize about every possible pattern that could be in the data and think to check it. Doing something that just asks what we see rather than why is the right angle to take there. Plus, sometimes seeing patterns in data can give us ideas for research questions that we can examine further in other data sources.

#### Why does Data Mining Have Difficulty Helping Theory?
- Data mining focuses on what's in the data, not *why* its in the data. 
- Its fantastic at revealing correlations - patterns in the data of how variables we've observed have varied together in the past - but the correlations it uncovers may have little to do with causality, or an understanding of why those variables move together. 
- Data mining is well-equipped to find the relationship but poorly-equipped to tell us why that relationship is there.
- Another reason is that, because it’s so focused on the data, data mining doesn’t really deal in abstraction.

#### False Positives
False positives are another reason why data mining can be dangerous. There are ways of avoiding false positives while doing data mining - this is something they worry about a lot in data science and have a lot of tools for. Data mining isn’t bad. It’s just bad as a final step if you’re trying to explain the world.

### Where do Research Questions Come From?
Research questions can come from lots of places. There are two steps in this process: 
1. thinking about theory
2. coming up with a research question. 

Either one can come first.

Perhaps it begins with theory. With the theory in place, the process continues with our hypothesis:

> *“if this is how the world works, what would I expect to see in the world?”*

Our theories might lead to research questions. These research questions tel us a hypothesis to test such that the result of that test tells us something about the theory. 


Let’s be honest, sometimes research questions also come from opportunity. Have a neat data set? Think about what data is available to you and whether any related research questions or theories come to mind.

### How do you know if you've got a good research question?
But is it really a good one? Just a few things to check before you get too far into the process:
1. Consider Potential Results. 
- A good way to double-check the relationship between your research question and your theory is to consider the potential answers you might get. 
- Then, imagine what kind of sense you’d make of that result, or what conclusion you would draw. Let’s say you find that students do tend to work harder in school when they’re paid for good grades. What would this tell us about how students respond to incentives? Or let’s say you find that students don’t work harder when they’re paid. What would that tell us about how students respond to incentives? 
- If you can’t say something interesting about your potential results, that probably means your research question and your theory aren’t as closely linked as you think! Let’s say we do find that kids who happen to play video games are more aggressive. Can we take that result and claim that video games are a cause of aggression? Not really, for the reasons we’ve discussed previously. So maybe that research question really isn’t linked to that theory very well.

2. Consider Feasibility
- A research question should be a question that can be answered using the right data, if the right data is available. But is the right data available? 
- If answering your research question is possible but requires following millions of people repeatedly for decades, or trying to measure something that’s really hard to measure accurately, like trying to get people to remember what they had for lunch three years ago, or getting access to the private finances of thousands of unwilling people, then that research question might not be feasible. 
- While sometimes you can get around these problems with a clever design, you might want to consider going back to the drawing board.

3. Consider Design
- A research question can be great on its own, but it can only be so interesting without an answer. So, an important part of evaluating whether you have a workable research question is figuring out if there’s a reasonable research design you can use to answer it. Figuring out whether you do have a reasonable research design is the topic of the rest of this book.

4. Keep it Simple
- Answering any research question can be difficult. Don’t make it even harder on yourself by biting off more than you can chew! 
- A common mistake is to bundle a bunch of research questions into one. “What are the determinants of social mobility?” I.e., how someone can move from one social class to another throughout their lifetime. There are many determinants of social mobility. You’re unlikely to answer that question well. Instead try “Is birth location a determinant of social mobility?” For another example, how about the question “How was the medium of painting affected by the Italian renaissance?” In a million ways! You’ll get lost and do a poor job on a bunch of minor pieces instead of getting at the whole. Instead maybe “What similar characteristics are there among the countries that adopted the use of perspective in painting most quickly?”

So, consider feasibility, scale, and design. Keep it simple, and think about whether the results you might likely see would tell you anything interesting about the world. After all, learning something interesting and new about the world is our goal!

___

### Chapter 3 - Describing Variables
This chapter will be all about how to describe a variable. It turns out that empirical research questions really come down entirely to **describing the density distributions of statistical variables.**

- A **variable**, in the context of empirical research, is a bunch of observations of the same measurement.
- Successfully describing a variable means being able to take those observations and clearly explain what was observed

#### 3.2: Types of Variables
The first step in figuring out how to describe a variable is figuring out what kind of variable it is. In general, the most common kinds of variables you will encounter are:
- **Continuous Variables:** Continuous variables are variables that could take any value (perhaps within some range).
- **Count Variables:** Count variables are those that, well, count something. Perhaps how many times something happened or how many of something there are.
- **Ordinal Variables:** Ordinal variables are variables where some values are “more” and others are “less”.
- **Categorical Variables:** Categorical variables are variables recording which category an observation is in. 
- **Binary Variables:** Categorical variables that only take two values. Often, these values are “yes” and “no.”

#### 3.3: The Distribution
Once we have an idea of what kind of variable we’re dealing with, the next step is to look at the **distribution** of that variable. A variable’s **distribution** is a description of how often different values occur.
- When it comes to **categorical** or **ordinal** variables, the variable’s distribution can be described by simply giving the percentage of observations that are in each category or value. The full distribution can be shown in a frequency table or bar graph, which just shows the percentage of the sample or population that has each value.
- **Continuous** variables are a little trickier. We can’t just do a frequency table for continuous variables since it’s unlikely that more than one observation takes any specific value. For **continuous** variables, distributions are described not by the probability that the variable takes a given value, but by *the probability that the variable takes a value close to that one.* One common way of expressing the distribution of a continuous variable is with a histogram.
- With a continuous variable we can go one step further than a histogram all the way to a density. A density shows what would happen to a histogram if the bins got narrower and narrower. When we have a density plot, we can describe the probability of being in a given range of the variable by seeing how large the area underneath the distribution is.
    - Once you have the distribution of the variable, that’s really all you can say about it until you start to incorporate how it relates to other variables. 

#### 3.4 Summarizing the Distribution
Once we have the variable’s distribution, we can turn our attention to summarizing that variable. The whole distribution might be a bit too much information for us to make any use of, especially for continuous variables. So our goal is to pick ways to take the *entire* distribution and produce a few numbers that describe that distribution pretty well.
- Probably the most well-known example of a single number that tries to describe an entire distribution is the mean.
    - So what the mean is actually doing is looking at the distribution of the variable and summarizing it, boiling it down to a single number. 
    - What is that number? The mean is supposed to represent a central tendency of the data - it’s in the middle of what you might get. 
    - More specifically, it tries to produce a representative value.
- If the goal is to describe the distribution to someone, why bother doing a calculation of the mean when we could just tell people about the distribution itself?
    - The *X*th percentile of a variable’s distribution is the value for which X% of the observations are less. We can see percentiles on our distribution graphs. We can actually describe the entire distribution perfectly this way. Pretty soon we'll have mapped the entire distribution by just shading in a little more each time. So percentiles are a fiarly direct way of describing a distribution.
- There are a few percentiles that deserve special mention.
    - The first is the **median**, or the 50th percentile. Like the mean, the **median** is measuring a central tendency of the data. Instead of trying to produce a representative value, like the mean does, the **median** gives a representative observation. The median is generally used over the mean when you want to describe what a typical observation looks like, or when you have a variable that is highly skewed, with a few really big observations.
    - The other two percentiles to focus on are the **minimum**, or the 0th percentile, and the **maximum**, or the 100th percentile. These are the lowest and highest values the variable takes.
- We need ways to describe variation in addition to central tendencies or percentiles. 
    - The way that variation shows up in a distribution graph is in how wide the distribution is. 
    - **Low variation:** If the distribution is tall and skinny, then all of the observations are scrunched in very close to the mean.
    - **High variation:** If it’s flat and wide, then there are a lot of observations in those fat “tails” on the left and right that are far away from the mean.

There are quite a few ways to describe variation. Some of them, like the mean, focus on values, and others, like the median and percentiles, focus on observations.
- **Variance** is a measure of variation that focuses on values and is derived from the mean. To calculate the variance in a sample of observations of our data, we:
1. Find the mean
2. Subtract the mean from each value
3. Square each of these values
4. Add them up
5. Divide by the number of observations minus 1
- The bigger the variance is, the more variation there is in that variable. iIn steps 4 and 5 previously that we’re sort of taking a mean. But the thing we’re taking the mean of is squared variation around the actual mean. So any observations that are far from the mean get squared - making them even bigger and count for more in our mean! In this way we get a sense of how far from the mean our data is, on average.
- One downside of the variance is that it’s a little hard to interpret, since it is in “squared units.” So we often convert the variance into the **standard deviation** by taking the square root of it to get us back to our original units.
- Figuring out how much variation one standard deviation is can be kind of tricky, and largely just takes practice and intuition. But a graph can help.

We can also compare percentiles to see how much a variable varies. All we have to do is pick a percentile above the median, and a percentile below the median, and see how different they are. The range can be very sensitive to really big observations, so its not a great measure. Instead, the most common percentile-based measure of variation you’ll tend to see is the **interquartile range**, or **IQR**. This gives the difference between the 75th percentile and the 25th percentile.

The IQR is handy for a few reasons: 
- First, you know that the value given by the IQR covers exactly half of your sample. So for the half of your sample closest to the median, the IQR gives you a good sense of how close to the median they are. 
- Second, unlike the variance, the IQR isn’t very strongly affected by big tail observations. So, as always, it’s a good way of representing observations rather than values.

**Skew** describes how the distribution leans to one side or the other.
- Distributions with a heavy right tail but no big left tail have a "right skew"
- Similarly, a distribution with lots of observations in the left tail would have a left skew
- A distribution with similar tails on both sides is symmetric.

Skew can be an important feature of a distribution to describe. It can also give us problems if we’re working with means and variances, since those really-huge values will affect any measure that tries to represent values. One way of handling skew in data is by **transforming** the data. If we apply some function to the data that shrinks the impact of those really-big observations, the mean and variance work better. A common transformation in this case is the **log transformation**, where we take the natural logarithm of the data and use that instead. This can make the data much better-behaved.

One reason the natural log transformation is so popular is that the transformed variable has an easy interpretation:
- A log increase of $0.1$ translates to approximately a $0.1 \times 100 = 1\% $ increase in the original variable.
- So an increase in log income from 10.00 to 10.02 in log terms means a $(10.02 - 10.00) \; = \; .02 \approx 2\%$ increase in income itself. 
- This approximation works pretty well for small increases like .01 or .02, but it starts to break down for bigger increases like, say, .2. Anything above .1/10% or so and you should avoid the approximation.

There might be trouble brewing if you take the log and it still looks skewed. This can be the case when you have fat tails, i.e., observations far away from the mean are very common. When you have fat tails on one side but not the other, this can make your data very difficult to work with indeed.

#### Theoretical Distributions
Statistics makes a very clear distinction between the truth and the data we’ve collected. But isn’t the data truly what we’ve collected? Well, sure, but what it is supposed to represent is some broader truth beyond that. We’ll need to pair our observed distributions, the ones we’ve been talking about so far in this chapter, with theoretical distributions of how data behaves under different versions of the truth.

**Quick Notation**
- English/Latin letters represent data. So $x$ might be a variable of actual observed data. That’s our 1000 surveys with parents about their kids’ sharing ages.
- Modifications of English/Latin letters represent calculations done with real data. A common way to indicate "mean" is to use a bar on top of the letter so $\bar{x}$ is the mean of $x$ we collected in our data. 
- Greek letters represent the *truth*. We don’t know what actual values these take, but we can make assumptions. ertain Greek letters are commonly used for certain kinds of truth:
    - $\mu$ commonly indicates some sort of mean.
    - $\sigma$ the standard deviation
    - $\rho$ for correlation
    - $\beta$ for regression on coefficients
    - $\epsilon$ for "error terms"
- Modifications of Greek letters represent our *estimate of the truth*. We don’t know what the truth is, but we can make our best guess of it.
    - The most common way to represent a guess is to put a "hat" on it:
        - So $\hat{\mu}$ is my estimate of what I think $\mu$ is. 
        - If the way that I plan to estimate $\mu$ is by taking the mean of $x$, then I would say $\hat{\mu} \; = \; \bar{x}$ 


The **theoretical distribution** is what generated your data. That’s actually a good way to think about theoretical distributions. They’re the distribution of all the data, even the data you didn’t actually collect, and maybe could never actually collect! 

If we want to learn about the average age children share at, the place that data comes from is the theoretical distribution. Remember, we don’t really care about the mean in our observed data, $\bar{x}$, we care about the true average $\mu$. 
- The reason we bother gathering data in the first place is because it will let us make an estimate $\hat{\mu}$ about what the theoretical distribution it came from is like.
- The second thing this “infinite observations” fact tells us is that the more observations we have, the better a job our observed data will do at matching that theoretical distribution.This means as we get more and more observations, we’re going to do a better and better job of getting an observed distribution that matches the theoretical one that we sampled the data from.

There are infinite different theoretical distributions, but some pop up in applied work often. There are some well-known distributions that are applied over and over again. If we think that our data follows one of these distributions we’re in luck, because it means we can use that theoretical distribution to do a lot of work for us!

The first to cover is the normal distribution. The normal distribution is symmetric (i.e., the left and right tails are the same size, there’s no skew/lean). The normal distribution often shows up when describing things that face real, physical restrictions, like height or intelligence.
- The normal distribution also pops up a lot when looking at aggregated values.
    - Income might have a strong right skew, but if we take the mean of income in one sample of data, aggregating across observations, and then again in another sample of data, aggregating across different observations, and then again and again in different samples, the *distribution of the mean across different samples* would have a normal distribution.
    - The normal distribution technically has infinite range, meaning that every value is possible, even if unlikely.

The second is a bit of a cheat, and it’s the log-normal distribution. The log-normal distribution has a heavy right skew, but once we take the logarithm of it, it turns out to be a normal distribution! How handy.
- The log-normal is a very convenient version of a skewed distribution, since we can take all the skew out of it by just applying a logarithm.
- Heavily skewed data comes up all the time in the real world. Anything that’s unequally distributed and that doesn’t have a maximum possible value is generally skewed (income, wealth…) When we see a skewed distribution, we tend to hope it’s log-normal for convenience reasons. So it’s a good idea, after taking the log of a skewed variable, to look at its distribution to confirm that it does indeed look normal.

How can we use our empirical data to learn about the theoretical distribution? 

Remember, our real reason for looking at and describing our variables is because we want to get a better idea of the theoretical distribution. We’re not really interested in the values of the variable in our sample, we’re interested in using our sample to find out about how the variable behaves in general.

We can, if we like, take our sample and look at its distribution (as well as its mean, standard deviation, and so on), figure that’s the best guess we have as to what the theoretical distribution looks like, and go from there.

One thing that is a bit easier to do is to learn whether certain theoretical distributions are unlikely. Maybe we can’t figure out exactly what the theoretical distribution is, but we can rule some stuff out. How can we figure out how likely a certain theoretical distribution is? We follow these steps:
1. Choose some description of the theoretical distribution - its mean, its median, its standard deviation, etc. Let’s use the mean as an example.
2. Use the properties of the theoretical distribution and your sample size to find the theoretical distribution of that description in random samples - means generally follow a normal distribution, and the standard deviation of that normal distribution is smaller for bigger sample sizes.
3. Make that same description of your observed data - so now we have the distribution of our theoretical mean, and we have the actual observed mean.
4. Use the theoretical distribution of that description to find out how unlikely it would be to get the data you got - if the theoretical distribution of the mean we’re looking at has mean 1 and standard deviation 2, and our observed mean is 1.5, we’re asking “how likely is it that we’d get a 1.5 or more from a normal distribution with mean 1 and standard deviation 2?”
5. If it’s really unlikely, then you probably started with the wrong theoretical distribution, and can rule it out. If we’re doing statistical significance testing, we might say that our observed mean is “statistically significantly different from” the mean of the theoretical distribution we started with.

We could also frame all of this in terms of hypothesis testing. Following the same steps, we can say:
- Step 1: Our null hypothesis is that the mean is 90. Our alternative hypothesis is that the mean is not 90.
- Step 2: Pick a test statistic with a known distribution. Means are distributed normally, so we might use a Z-statistic, which is for describing points on normal distributions.
- Step 3: Get that same test statistic in the data.
- Step 4: Using the known distribution of the test statistic, calculate how likely it is to get your data’s test statistic, or something even more extreme ($p$-value).
- Step 5: Determine whether we can reject the null hypothesis. This comes down to your threshold ($\alpha$), how unlikely does your data’s test statistic need to be for you to reject the null? A common number is 5%. If that's your threshold, then if step 4 gave you a lower $p$-value than your $\alpha$ threshold, then that's too unlikely for your, and you can reject the null hypothesis that the mean is 90. 

This section describes what hypothesis testing actually is and what it’s for. Statistical significance is not a marker of being correct or important. It’s just a marker of being able to reject some theoretical distribution you’ve chosen. That can certainly be interesting. At the very least, we’ve narrowed down the likely list of ways that our data could have been generated. And that’s the whole point!

## Chapter 4 - Describing Relationships

### 4.1 What is a Relationship?
For most research questions, we are not just interested in the distribution of a single variable. Instead, we are interested in the *relationship* we see in the data between two or more variables. The relationship between two variables shows you *what learning about one variable tells you about the other*.

For example, take height and age among children: 
- Generally, the older a child is, the taller they are. So, learning that one child is thirteen and another is six will give you a pretty good guess as to which of the two children is taller.
    - We can call the relationship between height and age positive, meaning that for higher values of one of the variables, we expect to see higher values of the other, too (more age is associated with more height).
    - There are also negative relationships, where higher values of one tend to go along with lower values of the other (more age is associated with less crying).
    - There are also null relationships where the variables have nothing to do with each other (older children aren’t any more or less likely to live in France than younger children).

A scatterplot is a basic way to show all the information about a relationship between two continuous variables, like the density plots were for a single continuous variable in Chapter 3. They're usually a great place to start describing a relationship. Scatterplots imply two things beyond what they actually show.
- The bad one is that it’s very tempting to look at a relationship in a scatterplot and assume that it means that the $x$-axis causes the $y$-axis.
- The good one is that it encourages us to use the scatterplot to imagine other ways of describing the relationship that might give us the information we want in a more digestible way.

### 4.2 Conditional Distributions
A conditional distribution is the distribution of one variable given the value of another variable.

Let’s start with a more basic version - conditional probability:
- The probability that someone is a woman is roughly 50%. 
- But the probability that someone who is named Sarah is a woman is much higher than 50%. 
- You can also say “among all Sarahs, what proportion are women?” We would say that this is the “probability that someone is a woman conditional on being named Sarah.”
- Learning that someone is named Sarah changes the probability that we can place on them being a woman.

Conditional distributions work the same way, except that this time, instead of just a single probability changing, an entire distribution changes. This also applies to categorical variables.

### 4.3 Conditional Means
With the concept of a conditional distribution under our belt, it should be clear that we can then calculate any feature of that distribution conditional on the value of another variable.
- What’s the 95th percentile of vitamin E taking overall and for smokers? What’s the median? 
- What’s the standard deviation of mortality for people who take 90th-percentile levels of vitamin E, and for people who take 10th-percentile levels?

While all those possibilities remain floating in the air, we will focus on the conditional mean. Given a certain value of $x$, what do I expect the mean of $y$ to be?

Once we have the conditional mean, we can describe the relationship between the two variables fairly well.
- If the mean of $Y$ is higher conditional on a higher value of $X$, then $Y$ and $X$ are positively related. 
- Going further, we can map out all of the conditional means of $Y$ for each value of $X$, giving us the full picture of how the mean of one variable is related to the values of the other.

In some cases, this is easy to calculate. If the variable you are conditioning on is discrete (or categorical), you can just calculate the mean of all observations with that value. 

Things get a little more complex when you are conditioning on a continuous variable. 

After all, I can’t give you the proportion taking vitamin E among those making $84,325 per year because there’s unlikely to be more than one person with that exact number. And lots of numbers would have nobody at all to take the mean over! 

There are two approaches we can take here:
- One approach is to use a *range* of values for the variable we're conditioning on rather than a single value. 
- Another is to use some sort of shape or line to fill in those gaps with no observations. 

Let’s focus first on using a range of values. 
- Since BMI is continuous, I’ve cut it up into ten equally-sized ranges (bins) and calculated the proportion taking vitamin E within each of those ranges. 
- Cutting the data into bins to take a conditional mean isn’t actually done that often in real research, but it gives a good intuitive sense of what we’re trying to do when we use other methods later.
- Those same ranges can be graphed. The flat lines reflect we are assigning the same mean to every observation in that range of BMI values. They show the mean conditional on being in that BMI bin. We see from this that BI has a positive relationship wtih taking vitamin E up until the 70+ ranges, at which point the conditional means drop. 

Of course, while this approach is simple and illustrative, it’s also fairly arbitrary. Instead, we can use a range of $X$ values to get conditional means of $Y$ using *local* means, where we calculate the conditional mean of $Y$ at a value (for example $X = 25$), we take the mean of $Y$ for all observations with $X$ values near 2.5. 

A common way to do this is with a LOESS curve, also known as a LOWESS. LOESS provides a local prediction, which it gets by fitting a different shape for each value on the $X$ axis, with the estimation of that shape weighting very-close observations more than kind-of-close observations. The end result is nice and smooth.

### 4.4 Line Fitting
Showing the mean of $Y$ among local values of $X$ is valuable, and can produce a highly detailed picture of the relationship between $X$ and $Y$. But it also has limitations. There might still be gaps in your data it has trouble filling in, for one. Also it can be hard sometimes to concisely describe the kind of relationship you see. Enter the concept of line-fitting or regression. 

Instead of thinking locally and producing estimates of the mean of $Y$ conditional on values of $X$, we can assume that the underlying relationship betwen $X$ and $Y$ can be represented by some sort of shape. In basic forms of regression, that shape is a straight line, For example, the line

$Y \; = \; X + 4X$

Tells us that the mean of $Y$ conditional on say, $X = 5$ is $3 + 4(5) = 23$. It also tells us that the mean of $Y$ conditional on a given value of $X$ would be 4 higher if you made it conditional on a value of $X$ one unit higher. This approach has real benefits: 
- It gives us the conditional mean of $Y$ for any value of $X$ we can think of, even if we don't have data for that specific value. 
- It also lets us very cleanly describe the relationship between $Y$ and $X$.
- If the slope coefficient on $X$ is positive, then $X$ and $Y$ are positively related.
- If its negative, they're negatively related. 

There are pragmatic upsides for using a fitted line. There are more upsides in statistical terms in using a line-fitting procedure to estimate the relationship.
- Since the line is estimated using all the data, rather than just local data, the results are more precise. 
- Also, the line can be easily extended to include more than one variable.

There is a downside as well:
- The biggest downside is that fitting a line requires us to *fit* a line. We need to pick what kind of shape the relationship is (straight, curved, wobbly?).
- If the shape is wrong to start with, then our conditional mean will be all wrong.

**Ordinary Least Squares (OLS) is the most well-known application of line-fitting. OLS picks the line that gives the lowest sum of squared residuals. 

A **residual** is the difference between an observation’s actual value and the conditional mean assigned by the line.

- Take that $Y = 3 + 4X$ example. We determined that the conditional mean of $Y$ when $X = 5$ is $3 + 4(5) = 23$.
- But what if we see someone in the data with $X = 5$ and $Y = 25$? 
- Well then their residual is $25 - 23 = 2$. OLS takes that number, squares it to a 4, then adds up all the predictions across all your data.
- Then it picks the values of $\beta_0$ and $\beta_1$ in the line $Y = \beta_0 + \beta_1X$ that make the sum of squared residuals as small as possible.

How does this work? It takes advantage of information about how the two variables move together or apart, encoded in the *covariance*. 

If you recall the variance from Chapter 3, you’ll remember that to calculate the variance of $X$, we: 
- (a) subtracted the mean of $X$ from $X$, 
- (b) squared the result,
- (c) added up the result across all the observations, and 
- (d) divided by the sample size minus one. 

The resulting variance shows how much a variable actually varies.

The covariance is the exact same thing, except that in step (a) you subtract the mean from two separate variables, and in step (b) you multiply the result from one variable by the result from the other. 

The resulting covariance shows how much two variables move together or apart. 
- If they tend to be above average at the same time or below average at the same time, then multiplying one by the other will produce a positive result for most observations, increasing the covariance. 
- If they have nothing to do with each other, then multiplying one by the other will give a positive result about half the time and a negative result the other half, canceling out in step (c) and give you a covariance of 0.

How does OLS use covariance to get the relationship between $X$ and $Y$? It just takes the covariance and divides it by the covariance of $X$. (i.e., $cov(X,Y)/var(X)$). 

This is roughly saying "of all the variation in $X$, how much of its varies along with $Y$?. Then, once it has its slope, it picks an intercept for the line that makes the mean of the residuals (not the squared residuals) $0$ (the conditional mean is at least right on average.)

The result from OLS is then a line with an intercept and a slope (like $Y = 3 + 4X$). You can plus in a value of $X$ to get the conditional mean of $Y$. Crucially, you can describe the relationship between the variables using the slope. 

Sometimes we may find it useful to rescale the OLS result. This brings us to the concept of **correlation**. 
- Correlation, specifically Pearson’s correlation coefficient, takes this exact concept and just rescales it, multiplying the OLS slope by the standard deviation of $X$ and dividing it by the standard deviation of $Y$. 
- This is the same as taking the covariance between $X$ and $Y$ and dividing by both the standard deviation of $X$ and the standard deviation of $Y$.
- We lose the ability to interpret the slope in terms of units of $X$ and $Y$, but gain the ability to easily tell how strong the relationship is. 

The correlation coefficient can only range from $-1$ to $1$, and the interpretation is the same no matter what units the original variables were in. 
- The closer to $-1$ it is, the more strongly the variables move in opposite directions.
- The closer to $1$ it is, the more strongly the variables move in the same direction (upward slope).

For the vitamin E and BMI example. OLS estimates the line

$VitaminE \; = \; \beta_0 + \beta_1 BMI$

and selects the best-fit values of $\beta_1$ and $\beta_2$ to give us

$VitaminE \; = \; .110 + .002BMI$

So for a one-unit increase in BMI we'd expect a .002 increase in the conditional mean of vitamin E. 

Then, since the standard deviation of taking vitamin E is .369 and the standard deviation of BMI is 6.543, the Pearson correlation between the two is 

$.002 \times 6.543 / .369 = .355$

OLS doesn't have to be a straight line, it just needs to fit a line that is "linear in the coefficients", meaning that the slope coefficients don't have to do anything wilder than being multiplied by a variable.

The 2nd hero is **nonlinear regression** (which takes many forms), but often in the form 

$Y \; = \; F(\beta_0 + \beta_1X)$

where $F$ is some function, depending on what you're doing. 

Nonlinear regression is commonly used when $Y$ can only take a limited number of values.

### Conditional Conditional Means, a.k.a. “Controlling for a Variable”
Let us enter the land of the unexplained. By which I mean the **residuals**.

When you get the mean of $Y$ conditional on $X$, you're splitting each observation into two parts:
1. The part *explained by* $X$ (the conditional mean)
2. The part *not explained by* $X$ (the residual)

We can think of the **residual** as the *part of $Y$ that has nothing to do with $X$*. 

There are a number of uses for the residual. Maybe we just want to know the variation in $Y$, but we want to know how much variation in $Y$ that is not explained by $X$. Looking at the residuals would answer exactly that question. 

Things get really interesting when we look at the residuals of two variables at once. What if we take the explained part out of two different variables? For example (with $X, Y, Z$)
1. Get the mean of $Y$ conditional on $Z$
2. Subtract out that conditional mean to get the residual of $Y$. Call this $Y^R$. 
3. Get the mean of $X$ condtional on $Z$
4. Subtract out that conditional mean to ge tthe residual of $X$. Call this $X^R$.
5. Describe the relationship between $Y^R$ and $X^R$.

Since $Y^R$ and $X^R$ have the parts of $Y$ and $X$ that can be explained with $Z$ removed, the relationship between $Y^R$ and $X^R$ is the *part of the relationship between $Y$ and $X$ that is not explained by $Z$*.

In doing this, we are taking out all the varaition related to $Z$, in effect not allowing $Z$ to vary. This is why we call this process "controlling for" $Z$.

Let’s take our ice cream and shorts example. We see that days where more people eat ice cream also tend to be days where more people wear shorts. But we also know that the temperature outside affects both of these things.
- If we really want to know if ice cream-eating affects shorts-wearing, we would want to know how much of a relationship is there between ice cream and shorts that isn’t explained by temperature?
- So we would get the mean of ice cream conditional on temperature, and then take the residual, getting only the variation in ice cream that has nothing to do with temperature.
- Then we would take the mean of shorts-wearing conditional on temperature, and take the residual, getting only the variation in shorts-wearing that has nothing to do with temperature.
- Finally, we get the mean of the shorts-wearing residual conditional on the ice cream residual. If the shorts mean doesn’t change much conditional on different values of ice cream eating, then the entire relationship was just explained by heat! 
- If there’s still a strong relationship there, maybe we do have something.

The easiest way to take conditional conditional means is with regression. Regression allows us to control for a variable by simply adding it to the equation. Now we have "multivariate" regression. So instead of:

$Y = \beta_0 + \beta_1X$

we just use

$Y = \beta_0 + \beta_1X + \beta_2Z$

the OLS estimate for $\$beta_1$ will automatically go through the steps of removing the conditional means and analyzing the relationship between $Y^R$ and $X^R$. 

we can do things conditional on more than on variable. So we could add $W$ and do:

$Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3W$

and now the $\beta_1$ that OLS picks will give us the relationship between $Y$ and $X$ conditional on *both* $Z$ and $W$.

So how does regression do this? 

The formula for multivariate OLS is

$(A'A)^-1 A'Y$, 

where $A$ is a matrix of all of the variables other than $Y$, including the $X$ we're interested in. In other words, it washes out the influence of all of the non-$X$ variables by dividing out a bunch of covariances.

## Chapter 5 - Identification

### 5.1 The Data Generating Process
One way to think about science generally is that scientists believe that there are regular laws that govern the way the universe works.

These laws are an example of a **“data generating process.”** The laws work behind the scenes, doing what they do whether we know about them or not. We can’t see them directly, but we do see the *data* that result from them.

We can see that if you let go of a ball, it drops to the ground. That’s our observation, our data. Gravity is a part of the data generating process for that ball. That’s the underlying law.

For example *gravity*:

If we have two objects that are distance $r$ apart, one with a mass of $m_1$ and another with a mass of $m_2$, then the force $F$ pulling them together is

$ F = G \frac{m_1 m_2}{r^2}$

where $G$ is the "gravitational constant". We can infer how the world works from our data because things like the above equation are part of the data generating process, or DGP. If two planets are moving around in space, their movement is actually *determined* by that equation, and then we *observe* their movement as data. 

The trick to data generating processes (DGPs) is that there are really two parts to them - the parts we know, and the parts we don’t. The parts we don’t know are what we’re hoping to learn about with our research.

The parts we already know about are just as important, though. The DGP combines everything we already know about a topic and its underlying laws. Then, we can use what we do know to learn new things. We can use what we know to figure out as much as we can about the data generating process. This will allow us to figure out reasons why we see the data we see, so we can focus just on the parts we’re interested in.

The first is the idea of looking for variation. The data generating process shows us all the different processes working behind the scenes that give us our data. But we’re only interested in part of that variation - in this case, it turned out to be the variation in income by hair color just among college students. How can we find the variation we need and focus just on that?

The second is the idea of identification. How can we use the data generating process to be sure that the variation we’re digging out is the right variation? Figuring out what sorts of problems in the data you need to clear away - like how we noticed that the non-college students dying their hair was giving us problems - is the process of identification.

### 5.2 Where's your Variation
how can we find the variation in the data that answers our question? The data as a whole is too messy - it varies for all sorts of reasons. But somewhere inside the data, our reason for variation is hiding. How can we get it out?

We have to ask what is the variation that we want to find? We need to use what we know about the data generating process to learn a little more.

The task of figuring out how to answer our research question is really the task of figuring out where your variation is. It’s unlikely that the variation in the raw data answers the question you’re really interested in. So where is the variation that does answer your question? How can you find it and dig it out? What variation needs to be removed to unearth the good stuff underneath?

That process - finding where the variation you’re interested in is lurking and isolating just that part so you know that you’re answering your research question - is called identification.

### Identification
Identification is the process of figuring out what part of the variation in your data answers your research question. It’s called identification because we’ve ensured that our calculation identifies a single theoretical mechanism of interest. In other words, we’ve gotten past all the misleading clues and identified our culprit.

A research question takes us from theory to hypothesis, making sure that the hypothesis we’re testing will actually tell us something about the theory. Identification takes us from hypothesis to the data, making sure that we have a way of testing that hypothesis in the data, and not accidentally testing some other hypothesis instead.The process of closing off other possible explanations is also how we identify the answers to research questions in empirical studies.

Once we have an idea of what our data generating process (DGP) we will have a good idea of what work we need to do. 

Identification requires statistical procedures in order to properly get rid of the kinds of variation we don’t want. But just as important, it relies on theory and assumptions about the way that the world works in order to figure out what those undesirable explanations are, and which statistical procedures are necessary. Specifically, it relies on theory and assumptions about what the DGP looks like.

If we want to identify the part of our data that gives the answer to our research question, we must:
1. Using theory, paint the most accurate picture possible of what the data generating process looks like
2. Use that data generating process to figure out the reasons our data might look the way it does that don’t answer our research question
3. Find ways to block out those alternate reasons and so dig out the variation we need

### 5.4 Alcohol and Mortality
Let’s take a look at one major study, A. M. Wood et al. on the effects of alcohol on “all cause mortality” - basically, your chances of dying sooner. This study has more than 200 authors and studied the relationship between drinking and outcomes like mortality and cardiovascular disease among nearly 600,000 people. It was published in 2018 in the prominent medical journal The Lancet, and it has been used in discussions of how to set medical drinking guidelines, as in Connor and Hall. 

The study is very well-regarded! What it found is that there did not appear to be a benefit of small amounts of drinking. Further, it found that the amount of alcohol it took to start noticing increased risk for serious outcomes like mortality was at about 100 grams of alcohol per week, which is about a drink per day, and below current guidelines in some countries.

We know from previous sections in this chapter that in order to make sense of the data we need to think carefully about the data generating process. What is the data generating process? What leads us to observe people drinking? What leads us to observe them dying? What reasons might there be for us to see an association between alcohol and mortality?

Thankfully, the authors of the Woods et al. study did manage to deal with some of these alternate explanations. They were putting some thought into what the data generating process looks like. 

You may have noticed, for one, that Figure 5.5 actually doesn’t contain non-drinkers. They’ve been left out of the study because of the too-sick-to-drink and ex-alcoholic alternate explanations they want to block out. This is one reason why the study doesn’t find a positive effect of a little alcohol while other studies do - some of those other studies leave in the non-drinkers (oops!).

### Context and Omniscience
for most research questions, especially any questions that have a causal element to them, understanding context is incredibly important. If you don’t understand where your data came from, there’s no way that you’ll be able to block alternative explanations and identify the answer to your question.

Before attempting to answer a research question empirically, be sure to put in plenty of time understanding the context that the data came from and how things work there.

No research project is perfect. All we can hope to do is:
- Learn what we can about the context so that we don’t miss any hugely important part of the data generating process
- Be careful to acknowledge what assumptions we’re making, and think about how they might be wrong
- Try to spot gaps in our knowledge about the data generating process, and make some realistic guesses about what might be in that gap
- Not aim for perfection, but aim for getting as close as we can

If we try to be omniscient, we’ll always fail. So there’s no point in that. But there is a point in trying to be useful. For that we simply need to learn what we can, work carefully, and try to make our errors small ones.
